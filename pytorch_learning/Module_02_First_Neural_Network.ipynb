{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Your First Neural Network with `nn.Module`\n",
    "\n",
    "---\n",
    "## What Is a Neural Network, Really?\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "A neural network is a **simplified copy of how your brain works**. Your brain has billions of neurons. Each receives signals, checks \"is this strong enough to pass on?\", then fires or stays silent. Signals flow layer by layer: eyes â†’ visual cortex â†’ recognition â†’ decision.\n",
    "\n",
    "PyTorch does the same with numbers:\n",
    "- **Neurons** = numbers (activations)\n",
    "- **Connections** = weights â€” how strongly one neuron influences the next\n",
    "- **\"Strong enough?\"** = activation function (ReLU) â€” the firing threshold\n",
    "- **Learning from mistakes** = backpropagation + optimizer\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Each layer is: `output = activation(W @ input + b)`. Stack enough layers and the universal approximation theorem says you can approximate ANY function. Each layer extracts increasingly abstract features.\n",
    "\n",
    "**Level:** Beginner  \n",
    "**Duration:** ~2.5 hours  \n",
    "**Dataset:** Iris Flower Dataset ([Kaggle](https://www.kaggle.com/datasets/uciml/iris))  \n",
    "**Real-World Use Case:** Multi-class species classification from measurements\n",
    "\n",
    "## What You'll Learn\n",
    "- Building models with `nn.Module` and `nn.Sequential`\n",
    "- Activation functions: ReLU, Sigmoid, Softmax\n",
    "- Loss functions: CrossEntropyLoss, MSELoss\n",
    "- Optimizers: SGD, Adam\n",
    "- The standard PyTorch training loop pattern\n",
    "- Evaluating classification accuracy\n",
    "\n",
    "## The PyTorch Training Loop â€” Memorize This Pattern!\n",
    "\n",
    "```\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()     â† 1. Clear old gradients\n",
    "    predictions = model(X)    â† 2. Forward pass\n",
    "    loss = criterion(pred, y) â† 3. Compute loss\n",
    "    loss.backward()           â† 4. Backprop\n",
    "    optimizer.step()          â† 5. Update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Gathering all tools â€” like laying out textbooks before studying.\n",
    "# âš™ï¸ torch.nn = layer library | torch.optim = weight-update algorithms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1  Load & Explore the Iris Dataset\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "You're a botanist measuring 4 things about each flower. From just those 4 numbers, your brain must decide: which of 3 species is this? This is classification â€” the brain does it constantly (\"dog or cat?\").\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Multi-class classification: input `x âˆˆ â„â´` â†’ label `y âˆˆ {0,1,2}`. 150 labelled examples are the \"worked problems\" the model studies from.\n",
    "\n",
    "150 flowers Ã— 4 features â†’ 3 species  \n",
    "*Same dataset available at* [kaggle.com/datasets/uciml/iris](https://www.kaggle.com/datasets/uciml/iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Opening a textbook of 150 worked examples: measurements â†’ correct species answer\n",
    "# âš™ï¸ X_raw: (150,4) feature matrix | y_raw: integer class labels 0/1/2\n",
    "iris = load_iris()\n",
    "X_raw = iris.data           # (150, 4)  features\n",
    "y_raw = iris.target         # (150,)    labels 0/1/2\n",
    "class_names = iris.target_names\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "df = pd.DataFrame(X_raw, columns=feature_names)\n",
    "df['species'] = [class_names[i] for i in y_raw]\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(\"\\nClass distribution:\\n\", df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Flip through the textbook before studying â€” do the measurements visually separate species?\n",
    "# âš™ï¸ EDA: overlapping histograms = hard problem; separated = easy. Petals are most discriminative.\n",
    "# â”€â”€ Visualise feature distributions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7))\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800']\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), feature_names):\n",
    "    for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "        ax.hist(X_raw[y_raw == i, feature_names.index(feat)],\n",
    "                bins=15, alpha=0.6, color=color, label=name)\n",
    "    ax.set_title(feat)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Iris Feature Distributions by Species\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Data Preparation\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Standardise units (all measurements on same scale), split into study pile (80%) and mock exam (20%), study in shuffled batches of 16 â€” prevents memorising card order.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`StandardScaler`: z-score normalisation, fit ONLY on train (no data leakage). DataLoader: mini-batches, shuffle each epoch. stratify: equal class proportions in both splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Split cards 80/20, shuffle, convert to brain format (tensors). Standardise to same scale.\n",
    "# âš™ï¸ Fit scaler on train only â€” same Î¼,Ïƒ applied to test to prevent leakage\n",
    "# â”€â”€ Train/test split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=42, stratify=y_raw\n",
    ")\n",
    "\n",
    "# â”€â”€ Standardize features (zero mean, unit variance) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)   # fit only on train!\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# â”€â”€ Convert to PyTorch tensors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "y_train_t = torch.LongTensor(y_train).to(device)\n",
    "X_test_t  = torch.FloatTensor(X_test).to(device)\n",
    "y_test_t  = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# â”€â”€ Wrap in DataLoader for mini-batch training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_ds     = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Train: {X_train_t.shape}   Test: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Building the Model â€” Three Approaches\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Each `nn.Linear(in, out)` is a group of neurons where every input connects to every output. `ReLU` is the firing threshold: \"pass signal only if positive.\" `Dropout` randomly silences 20% of neurons during training, forcing the brain to build redundant paths.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`nn.Sequential` = Unix pipe. Custom `nn.Module` = circuit board with explicit wiring. Required for skip connections, branching, shared weights. `Dropout(p=0.2)` = Bernoulli noise regularisation, active only in `train()` mode.\n",
    "\n",
    "### Approach A: `nn.Sequential` (quick, readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Assembly line: 4 measurements â†’ layer-by-layer transformation â†’ 3 species scores\n",
    "# âš™ï¸ No softmax at end â€” CrossEntropyLoss applies log-softmax internally (more stable)\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(4, 16),       # input: 4 features â†’ 16 hidden neurons\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),       # 16 â†’ 8\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 3)         # 8 â†’ 3 classes (raw logits, no softmax)\n",
    ").to(device)\n",
    "\n",
    "print(model_seq)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_seq.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach B: Custom `nn.Module` class (preferred for complex models)\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "`nn.Module` is the **brain circuit blueprint**. `__init__` lists components (layers); `forward(x)` wires them. `Dropout(p=0.2)` randomly silences 20% â€” forces robust redundant representations.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`nn.Module` auto-registers parameters, enables `train()`/`eval()` switching, provides `state_dict()`. Dropout = stochastic regularisation â€” active in `train()`, no-op in `eval()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Blueprint: 4 senses â†’ 16 detectors â†’ 8 patterns â†’ 3 species decisions\n",
    "# âš™ï¸ __init__: declare components | forward: define signal flow (the wiring diagram)\n",
    "class IrisNet(nn.Module):\n",
    "    \"\"\"Feed-forward network for Iris species classification.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=16, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,  hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)      # regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)                       # raw logits out\n",
    "        return x\n",
    "\n",
    "\n",
    "model = IrisNet().to(device)\n",
    "print(model)\n",
    "\n",
    "# Test with a dummy batch\n",
    "dummy = torch.randn(8, 4).to(device)\n",
    "out   = model(dummy)\n",
    "print(f\"\\nOutput shape for batch of 8: {out.shape}\")  # (8, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  Loss Function & Optimizer\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "**CrossEntropyLoss** = \"How confident were you in the WRONG answer?\" (big confidence + wrong = big regret). **Adam** = intelligent study strategy â€” remembers gradient history, adapts step size per weight. **ReduceLROnPlateau** = \"slow down when you're not improving.\"\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`CrossEntropyLoss` = `-log(p_correct)`, takes raw logits NOT softmax output. Adam maintains momentum + variance per parameter â€” adapts per-weight LR. `ReduceLROnPlateau` halves LR after `patience` epochs of stagnation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Loss = how wrong is the guess? Optimizer = how to fix it? Scheduler = when to slow down?\n",
    "# âš™ï¸ CrossEntropyLoss takes raw logits â€” DO NOT apply softmax before passing to it\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss  (takes raw logits, NOT probabilities)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer: adaptive learning rates (usually better than vanilla SGD)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Optional: learning rate scheduler (reduce lr when plateau)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5  Training Loop\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Every mini-batch: (1) wipe whiteboard, (2) make guess, (3) score it, (4) trace which neurons caused error, (5) adjust those neurons. After 150 rounds of this across hundreds of mini-batches, the brain has refined its flower knowledge.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`Î¸ â† Î¸ - lr * Adam(âˆ‡L)`. `model.train()` enables Dropout. `model.eval()` disables it. `@torch.no_grad()` skips gradient tape construction â€” ~30% faster for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  5-step ritual: wipe â†’ guess â†’ score â†’ trace error â†’ adjust. Repeat 150 rounds.\n",
    "# âš™ï¸ zero_grad() MUST precede backward() â€” gradients accumulate by default\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()           # enable dropout / batchnorm training mode\n",
    "    total_loss, correct = 0.0, 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()                    # â‘  clear gradients\n",
    "        logits = model(X_batch)                  # â‘¡ forward\n",
    "        loss   = criterion(logits, y_batch)      # â‘¢ loss\n",
    "        loss.backward()                          # â‘£ backprop\n",
    "        optimizer.step()                         # â‘¤ update\n",
    "\n",
    "        total_loss += loss.item() * len(X_batch)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, X, y, criterion):\n",
    "    model.eval()            # disable dropout\n",
    "    logits = model(X)\n",
    "    loss   = criterion(logits, y).item()\n",
    "    preds  = logits.argmax(dim=1)\n",
    "    acc    = (preds == y).float().mean().item()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "# â”€â”€ Run training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EPOCHS = 150\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    vl_loss, vl_acc = evaluate(model, X_test_t, y_test_t, criterion)\n",
    "    scheduler.step(vl_loss)\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(vl_loss)\n",
    "    history['val_acc'].append(vl_acc)\n",
    "\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"Train loss: {tr_loss:.4f} acc: {tr_acc:.3f} | \"\n",
    "              f\"Val loss: {vl_loss:.4f} acc: {vl_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6  Visualise Training\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Loss curve = learning diary. Rapid early drop â†’ brain learns obvious patterns fast. Flattening = harder patterns. Big gap between train/val = \"memorised cards but can't generalise\" (overfitting).\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Flat loss â†’ vanishing gradients or LR too low. Oscillating â†’ LR too high. Val loss rising while train drops â†’ overfitting â†’ add Dropout or reduce model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Left: fewer mistakes over time? Right: more correct guesses? Gap = overfitting?\n",
    "# âš™ï¸ Healthy training: train and val curves track each other closely\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'],   label='Validation')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curves'); ax1.legend()\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'],   label='Validation')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves'); ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7  Evaluation â€” Confusion Matrix\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Confusion matrix shows WHERE the brain got confused: \"when the flower was virginica, how many times did it say versicolor?\" Off-diagonal = errors. Setosa is easy (very distinct petals); versicolor/virginica are similar â†’ higher confusion.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`cm[i,j]` = count of true class `i` predicted as `j`. Precision/recall/F1 per class â€” more informative than raw accuracy for imbalanced problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Exam time: show unseen flowers, record every answer, build the error grid\n",
    "# âš™ï¸ model.eval() + no_grad() = inference mode; preds = argmax(logits)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True'); plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix â€” Iris Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8  Saving & Loading Models\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Saving `state_dict` = photographing all synaptic strengths to disk. Loading = restoring the brain from that snapshot â€” all learned knowledge returns instantly. Need BOTH the blueprint (class definition) AND the saved weights.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`state_dict()` = ordered dict of `{layer_name: weight_tensor}`. Save state_dict, not whole model â€” portable across PyTorch versions. `map_location=device` handles GPUâ†’CPU loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Photograph synaptic strengths â†’ save â†’ restore later with all knowledge intact\n",
    "# âš™ï¸ state_dict = lightweight dict of tensors; map_location handles device differences\n",
    "# â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Best practice: save only the state_dict (weights), not the entire model\n",
    "torch.save(model.state_dict(), 'iris_model.pth')\n",
    "print(\"Model saved to iris_model.pth\")\n",
    "\n",
    "# â”€â”€ Load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "loaded_model = IrisNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load('iris_model.pth', map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Verify\n",
    "with torch.no_grad():\n",
    "    _, acc = evaluate(loaded_model, X_test_t, y_test_t, criterion)\n",
    "print(f\"Loaded model accuracy: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9  Inference on New Samples\n",
    "\n",
    "### ðŸ§  Brain Analogy\n",
    "Real-world use: give the trained botanist-brain a NEW flower and ask \"what species?\" It returns probabilities: \"94% sure virginica.\" Confidence = how far from 0.5.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Inference: preprocess with the SAME scaler (same Î¼,Ïƒ) â†’ tensor â†’ `eval()` forward â†’ softmax â†’ argmax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  New flower â†’ trained brain makes its best guess using all learned knowledge\n",
    "# âš™ï¸ ALWAYS use the same scaler transform as training â€” wrong normalisation = wrong features\n",
    "def predict_species(measurements: list) -> str:\n",
    "    \"\"\"Predict Iris species from [sepal_len, sepal_wid, petal_len, petal_wid].\"\"\"\n",
    "    x = torch.FloatTensor(scaler.transform([measurements])).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(x)\n",
    "        probs  = torch.softmax(logits, dim=1)[0]\n",
    "        pred   = probs.argmax().item()\n",
    "    return class_names[pred], {n: f\"{p:.1%}\" for n, p in zip(class_names, probs.tolist())}\n",
    "\n",
    "\n",
    "# Test with known samples\n",
    "samples = [\n",
    "    ([5.1, 3.5, 1.4, 0.2], \"setosa\"),\n",
    "    ([6.1, 2.8, 4.7, 1.2], \"versicolor\"),\n",
    "    ([6.3, 3.3, 6.0, 2.5], \"virginica\"),\n",
    "]\n",
    "\n",
    "for features, true_label in samples:\n",
    "    pred, probs = predict_species(features)\n",
    "    status = 'âœ“' if pred == true_label else 'âœ—'\n",
    "    print(f\"{status} True: {true_label:12s} â†’ Predicted: {pred:12s} | {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10  Key Concepts Summary\n",
    "\n",
    "| Concept | PyTorch API | Notes |\n",
    "|---------|------------|-------|\n",
    "| Linear layer | `nn.Linear(in, out)` | Weight + bias |\n",
    "| Activation | `nn.ReLU()`, `nn.Sigmoid()` | After linear |\n",
    "| Dropout | `nn.Dropout(p)` | Regularization |\n",
    "| Loss | `nn.CrossEntropyLoss()` | Multi-class |\n",
    "| Optimizer | `optim.Adam(params, lr)` | Adaptive LR |\n",
    "| Train mode | `model.train()` | Enable dropout |\n",
    "| Eval mode | `model.eval()` | Disable dropout |\n",
    "| No gradient | `torch.no_grad()` | Faster inference |\n",
    "| Save | `torch.save(model.state_dict(), path)` | |\n",
    "| Load | `model.load_state_dict(torch.load(path))` | |\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Add **Batch Normalization** (`nn.BatchNorm1d`) after each hidden layer. Does it improve results?\n",
    "2. Try `optim.SGD` with `momentum=0.9` vs Adam. Which converges faster?\n",
    "3. Download the full Iris CSV from Kaggle and load it with `pd.read_csv` instead of sklearn.\n",
    "4. Plot the **decision boundary** for the first two features using a meshgrid.\n",
    "\n",
    "---\n",
    "**Next â†’** [Module 03: Data Pipelines with Dataset & DataLoader (Titanic)](./Module_03_Data_Pipeline_Titanic.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}