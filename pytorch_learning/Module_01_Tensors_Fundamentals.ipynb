{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: PyTorch Tensors & Fundamentals\n",
    "\n",
    "**Level:** Beginner  \n",
    "**Duration:** ~2 hours  \n",
    "**Real-World Use Case:** Understanding how numerical data flows through deep learning pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## The Big Picture â€” Two Analogies\n",
    "\n",
    "### ğŸ§  Human Brain Analogy\n",
    "Your brain processes reality by converting everything â€” sight, sound, touch â€” into **electrical signals**: streams of numbers firing across billions of neurons. PyTorch tensors are the same idea: they are the universal language in which a computer encodes *all* information before a neural network can think about it.\n",
    "\n",
    "- A **photo** becomes a 3D tensor of pixel brightness values `(3, 224, 224)` â€” like the retina converting light into nerve signals.\n",
    "- A **sentence** becomes a 2D tensor of word embeddings `(seq_len, 768)` â€” like the auditory cortex encoding sound into meaning.\n",
    "- The **network's weights** are tensors too â€” like the synaptic strengths between neurons, updated every time the network learns.\n",
    "\n",
    "> *Tensors are to PyTorch what neurons are to the brain â€” the fundamental unit everything is built from.*\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Think of a tensor as a **typed, multi-dimensional buffer in memory** â€” like a structured data array in a CAD system or a register file in a CPU. Before any computation can happen, an engineer must:\n",
    "1. **Allocate** the buffer (create the tensor)\n",
    "2. **Specify its format** (dtype, shape)\n",
    "3. **Place it on the right hardware** (CPU vs GPU)\n",
    "\n",
    "This module teaches you to do exactly that.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "- Creating and manipulating tensors (the core data structure in PyTorch)\n",
    "- Tensor operations: math, indexing, reshaping\n",
    "- Moving tensors to GPU\n",
    "- Autograd: automatic differentiation\n",
    "- Connecting PyTorch tensors to NumPy\n",
    "\n",
    "## Mental Model\n",
    "Think of a **Tensor** as a multi-dimensional array (like NumPy ndarray) that:\n",
    "1. Can run on GPU for massive speed gains\n",
    "2. Tracks computation history for automatic gradient calculation (backprop)\n",
    "\n",
    "```\n",
    "Scalar  â†’ 0D tensor  â†’ single number:   42             (one neuron's output)\n",
    "Vector  â†’ 1D tensor  â†’ list:            [1, 2, 3]      (one layer's activations)\n",
    "Matrix  â†’ 2D tensor  â†’ grid:            [[1,2],[3,4]]  (a weight layer W)\n",
    "Tensor  â†’ 3D+        â†’ e.g. RGB image:  (3, 224, 224)  (what the eye sends in)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1.0  Environment Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Brain: Before your brain does any thinking, it needs blood flow (energy) and\n",
    "#           the right neurotransmitters ready. This cell is our \"brain warm-up\" â€”\n",
    "#           confirming the tools (PyTorch, NumPy) and hardware (CPU/GPU) are alive.\n",
    "#\n",
    "# âš™ï¸ Engineer: Like checking power rails and firmware version before running\n",
    "#              a microcontroller. No point writing code if the runtime isn't ready.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")   # GPU = turbo mode\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device    : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Creating Tensors â€” Six Ways\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Before your brain can reason about something, it must **encode** it â€” convert the raw world into a form neurons can process. When you see the number \"3\", your visual cortex fires a specific pattern of neurons; that pattern *is* the brain's tensor.\n",
    "\n",
    "Creating a tensor in PyTorch is like **choosing how to encode information** before passing it into a network:\n",
    "- **From a list** â†’ like a person recalling a memory and writing it down precisely\n",
    "- **Zeros / Ones** â†’ like a blank or fully-activated neuron layer (initial state before learning)\n",
    "- **Random (randn)** â†’ like the random initial synaptic weights when a baby is born â€” noisy, but ready to be shaped by experience\n",
    "- **From NumPy** â†’ like translating a document between two languages that share memory (no copying needed!)\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Think of each creation method as a different **memory allocation strategy** for a typed buffer:\n",
    "\n",
    "| Method | Engineering Equivalent |\n",
    "|--------|------------------------|\n",
    "| `torch.tensor(list)` | Initialise buffer from known values (hard-coded data) |\n",
    "| `torch.zeros / ones` | `calloc` â€” allocate and zero-fill; safe known initial state |\n",
    "| `torch.rand / randn` | Random seed initialisation â€” like Xavier/He weight init in practice |\n",
    "| `torch.arange` | Loop counter array â€” evenly-spaced index buffer |\n",
    "| `torch.linspace` | Uniform sample grid â€” like frequency bins in an FFT |\n",
    "| `torch.from_numpy` | Shared-memory view â€” **zero-copy** pointer aliasing (changes in one reflect in the other!) |\n",
    "\n",
    "> **Key engineering insight:** `torch.from_numpy` does NOT copy data. Both the NumPy array and the tensor point to the same memory block. Modify one, and the other changes too â€” like two pointers aliasing the same address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ From Python lists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like manually writing down a memory: \"I saw 1, 2, 3, 4\"\n",
    "# âš™ï¸ Like hard-coding known register values at boot time\n",
    "t_from_list = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(\"From list:\\n\", t_from_list)\n",
    "\n",
    "# â”€â”€ Zeros / Ones / Random â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  zeros  â†’ a silent brain region (no activation yet)\n",
    "# ğŸ§  ones   â†’ a fully saturated/active region\n",
    "# ğŸ§  rand   â†’ random background neural noise (uniform)\n",
    "# ğŸ§  randn  â†’ natural neural noise â€” most activity near 0, rare extremes (bell curve)\n",
    "# âš™ï¸ zeros/ones â†’ calloc vs memset(1) â€” deterministic initial state\n",
    "# âš™ï¸ randn  â†’ the Xavier/He initialisation engineers use to prevent vanishing gradients\n",
    "zeros  = torch.zeros(3, 4)                    # 3Ã—4 filled with 0s\n",
    "ones   = torch.ones(3, 4)                     # 3Ã—4 filled with 1s\n",
    "rand   = torch.rand(3, 4)                     # uniform [0,1)\n",
    "randn  = torch.randn(3, 4)                    # standard normal N(0,1)\n",
    "\n",
    "# â”€â”€ Evenly-spaced sequences â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like a metronome ticking at fixed intervals â€” predictable, structured time steps\n",
    "# âš™ï¸ arange â†’ C-style loop index  |  linspace â†’ sample grid (like FFT frequency bins)\n",
    "arange = torch.arange(0, 10, step=2)          # like np.arange:   [0, 2, 4, 6, 8]\n",
    "linsp  = torch.linspace(0, 1, steps=5)        # like np.linspace: [0, .25, .5, .75, 1]\n",
    "\n",
    "print(\"\\nzeros:\\n\",  zeros)\n",
    "print(\"arange:\",    arange)\n",
    "print(\"linspace:\",  linsp)\n",
    "\n",
    "# â”€â”€ From NumPy â€” ZERO-COPY shared memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like two parts of the brain accessing the same long-term memory without\n",
    "#    duplicating it â€” efficient and always in sync.\n",
    "# âš™ï¸ Pointer aliasing: np_arr and t_np share the same RAM block.\n",
    "#    WARNING: modifying t_np will also mutate np_arr!\n",
    "np_arr  = np.array([1.0, 2.0, 3.0])\n",
    "t_np    = torch.from_numpy(np_arr)            # shares memory â€” no copy!\n",
    "print(\"\\nFrom NumPy:\", t_np)\n",
    "\n",
    "# Prove they share memory:\n",
    "np_arr[0] = 99.0\n",
    "print(\"After mutating np_arr[0]=99, t_np:\", t_np)  # t_np[0] also becomes 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Tensor Attributes\n",
    "\n",
    "Every tensor carries three key properties: **shape**, **dtype**, **device**.\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Think of a tensor as a group of neurons that carries metadata about itself:\n",
    "- **`shape`** â€” *how many neurons are in the group, and how they're arranged* (e.g. a visual cortex column is arranged in layers â€” 4 batches Ã— 3 channels Ã— 2 features)\n",
    "- **`dtype`** â€” *the precision of each neuron's signal* â€” float32 is like a standard nerve signal; int8 is a coarser, compressed approximation (like how your peripheral vision is lower resolution than your fovea)\n",
    "- **`device`** â€” *which part of the brain (or body) is doing the processing* â€” CPU is the deliberate prefrontal cortex; GPU is the fast, parallel cerebellum\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "A tensor's metadata is its **hardware spec sheet**:\n",
    "- **`shape`** â†’ array dimensions = how the buffer is laid out in memory (row-major, like a 3D grid)\n",
    "- **`dtype`** â†’ data type = `float32` costs 4 bytes/element, `int8` costs 1 byte â€” choose wisely for speed vs precision tradeoffs (quantisation!)\n",
    "- **`device`** â†’ compute unit = CPU (general-purpose, low-latency) vs GPU (high-throughput, high-latency per op but massively parallel)\n",
    "- **`numel()`** â†’ total allocation size = `shape[0] Ã— shape[1] Ã— ... Ã— shape[n]` â€” tells you the memory footprint\n",
    "\n",
    "> **Pro tip:** In production ML, engineers spend a lot of time reducing `dtype` (e.g. float32 â†’ float16 or int8) to fit more data into GPU memory without losing accuracy. This is called **quantisation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Imagine a 3D brain region: 4 people (batch), each with 3 sensory channels,\n",
    "#    each channel producing 2 feature values. That's x.shape = (4, 3, 2).\n",
    "# âš™ï¸ A 3-rank tensor of shape (4,3,2) â€” 24 float32 values = 24 Ã— 4 = 96 bytes in memory.\n",
    "x = torch.randn(4, 3, 2)   # 4 batches, 3 channels, 2 features\n",
    "\n",
    "print(\"Shape  :\", x.shape)          # torch.Size([4, 3, 2])  â† the memory layout\n",
    "print(\"ndim   :\", x.ndim)           # 3                      â† rank of the tensor\n",
    "print(\"dtype  :\", x.dtype)          # torch.float32          â† 4 bytes per element\n",
    "print(\"device :\", x.device)         # cpu                    â† where it lives\n",
    "print(\"numel  :\", x.numel())        # 4*3*2 = 24 elements total (= bytes/4 for float32)\n",
    "\n",
    "# â”€â”€ Changing dtype: trading precision for speed/memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like switching from HD to SD vision â€” less detail, but faster to process\n",
    "# âš™ï¸ float32 â†’ int32: drops fractional precision; saves no memory here (both 4 bytes),\n",
    "#    but int8 (not shown) would cut to 1 byte â€” used in quantised inference engines\n",
    "x_int = x.to(torch.int32)\n",
    "print(\"\\nAs int32 dtype:\", x_int.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  Essential Tensor Operations\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "When neurons communicate, they don't just pass raw signals â€” they **combine, scale, and summarise** them:\n",
    "- **Element-wise ops** (`a + b`, `a * b`) â†’ like two neurons that both sense the same stimulus and their signals are simply added or multiplied together at the next neuron (synaptic summation)\n",
    "- **Matrix multiplication** (`a @ b`) â†’ like a **full layer of neurons**: every input neuron talks to every output neuron, each connection weighted differently. This is *exactly* what a dense (linear) layer does: `output = W @ input`\n",
    "- **Aggregation** (`sum`, `mean`, `max`) â†’ like a region of the brain **summarising** its inputs into a single decision signal â€” e.g. the basal ganglia voting \"go\" or \"no-go\" based on the overall activity level\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "- **Element-wise ops** â†’ SIMD (Single Instruction, Multiple Data) vector operations â€” the CPU/GPU applies one instruction to all 32/64/128 elements simultaneously\n",
    "- **`a @ b` (MatMul)** â†’ the most important op in deep learning; on a GPU it maps to **GEMM** (General Matrix-Matrix Multiply), optimised using BLAS libraries (cuBLAS on CUDA). This single operation accounts for ~90% of compute in transformer models\n",
    "- **`argmax`** â†’ like a priority encoder in digital logic â€” outputs the *index* of the winning signal\n",
    "- **`dim=0` vs `dim=1`** â†’ reducing along a dimension collapses that axis; think of it as collapsing rows (dim=0) or columns (dim=1) of a spreadsheet\n",
    "\n",
    "> **Why dim matters:** `sum(dim=0)` sums across rows (result has shape of one row), `sum(dim=1)` sums across columns (result has shape of one column). Getting this wrong is one of the most common PyTorch bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "# â”€â”€ Element-wise arithmetic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Two neuron groups firing simultaneously â€” signals combine position-by-position\n",
    "# âš™ï¸ SIMD ops: each element processed in parallel, same instruction applied to all\n",
    "print(\"a + b:\\n\",  a + b)          # same as torch.add(a, b)\n",
    "print(\"a * b:\\n\",  a * b)          # element-wise multiply (NOT matrix multiply!)\n",
    "print(\"a ** 2:\\n\", a ** 2)         # squaring each activation â€” like a power amplifier\n",
    "\n",
    "# â”€â”€ Matrix multiplication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  THE core brain operation: every input neuron connects to every output neuron.\n",
    "#    a is the input activations (2Ã—2), b is the weight matrix (2Ã—2).\n",
    "#    Result[i,j] = dot product of row i of a with col j of b.\n",
    "# âš™ï¸ Maps to GEMM on GPU â€” the single most-optimised operation in deep learning.\n",
    "#    In a transformer: Q @ K.T is this operation, done billions of times per second.\n",
    "print(\"\\nMatMul a @ b:\\n\", a @ b)  # same as torch.matmul(a, b)\n",
    "\n",
    "# â”€â”€ Aggregation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  The brain compressing many signals into one summary statistic to make a decision\n",
    "# âš™ï¸ Reduction operations â€” collapse the tensor to a scalar or lower-rank tensor\n",
    "print(\"\\nSum:\",     a.sum())        # total energy across all neurons\n",
    "print(\"Mean:\",    a.mean())        # average activation level\n",
    "print(\"Max: \",    a.max())         # the most active neuron's value\n",
    "print(\"Argmax:\",  a.argmax())      # WHICH neuron was most active (its flat index)\n",
    "                                   # argmax([1,2,3,4]) â†’ 3  (index of 4)\n",
    "\n",
    "# â”€â”€ Axis-wise aggregation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Summarising along a specific brain axis:\n",
    "#    dim=0 â†’ \"what was the average signal across the batch of inputs?\" (column summary)\n",
    "#    dim=1 â†’ \"what was the total signal for each individual input?\" (row summary)\n",
    "# âš™ï¸ Think of it as collapsing a spreadsheet:\n",
    "#    dim=0 â†’ collapse rows â†’ result shape = (num_cols,)\n",
    "#    dim=1 â†’ collapse cols â†’ result shape = (num_rows,)\n",
    "print(\"\\nSum along rows (dim=0):\", a.sum(dim=0))  # [1+3, 2+4] = [4, 6]  (column sums)\n",
    "print(\"Sum along cols (dim=1):\", a.sum(dim=1))   # [1+2, 3+4] = [3, 7]  (row sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4  Indexing, Slicing & Reshaping\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Your brain doesn't attend to everything at once â€” it uses **selective attention** to focus on what matters:\n",
    "- **Indexing** (`x[1, 2]`) â†’ like your brain spotlight-focusing on a single pixel in your visual field â€” everything else is still there, but ignored\n",
    "- **Slicing** (`x[:2]`, `x[:, 2]`) â†’ like peripheral vision â€” you're processing a *region* of the scene, not a single point\n",
    "- **Reshape / View** â†’ like rearranging the furniture in a room without changing what's in it. The data stays the same; only the *layout* changes. Your brain does this when it re-contextualises information (e.g. a 3D object mentally rotated)\n",
    "- **unsqueeze / squeeze** â†’ like adding a \"batch dimension\" to a single thought â€” your brain can simultaneously hold \"what is this?\" (a single example) and \"how does this compare across many examples?\" (a batch)\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "- **Indexing** â†’ pointer arithmetic into a contiguous memory buffer: `x[i, j]` = `base_ptr + i * stride_0 + j * stride_1`\n",
    "- **Slicing** â†’ returns a **view** (not a copy!) â€” like a window into the same memory block with different start/stride\n",
    "- **`reshape` vs `view`** â†’ both reinterpret the same memory layout. `view` requires contiguous memory; `reshape` will copy if needed â€” similar to `reinterpret_cast` vs `memcpy` + cast\n",
    "- **`unsqueeze(dim)`** â†’ inserts a size-1 dimension â€” critical for **broadcasting** (next section). Used constantly to align tensor shapes for batch operations: `(3,)` â†’ `(1, 3)` makes it broadcastable over a batch\n",
    "- **`squeeze()`** â†’ removes all size-1 dimensions â€” like stripping redundant wrapper layers\n",
    "\n",
    "> **Memory insight:** Slicing and `view` create tensors that **share memory** with the original. Modifying the slice modifies the source. Call `.clone()` if you need an independent copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 3Ã—4 matrix from a range â€” like a 3-row spreadsheet with 4 columns\n",
    "# ğŸ§  Imagine 3 people, each described by 4 brain measurements\n",
    "# âš™ï¸ arange(1,13) creates [1..12]; reshape reinterprets the flat buffer as 3Ã—4 grid\n",
    "x = torch.arange(1, 13).reshape(3, 4)   # shape (3,4)\n",
    "print(\"Original:\\n\", x)\n",
    "\n",
    "# â”€â”€ Indexing â€” spotlight attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Zoom in on exactly one neuron's reading\n",
    "# âš™ï¸ Resolves to: base + 0*stride_0 = base (first row of the buffer)\n",
    "print(\"\\nRow 0      :\", x[0])             # first row    â†’ [1, 2, 3, 4]\n",
    "print(\"Col 2      :\", x[:, 2])           # entire 3rd column â†’ [3, 7, 11]\n",
    "print(\"x[1,2]     :\", x[1, 2])           # single element: row 1, col 2 â†’ 7\n",
    "print(\"Rows 0-1   :\\n\", x[:2])           # first two rows (slice = view, not copy!)\n",
    "\n",
    "# â”€â”€ Reshape / View â€” reorganise without copying â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Same 12 \"facts\", rearranged into a different mental grid\n",
    "# âš™ï¸ Both return views sharing the same memory â€” just different stride metadata\n",
    "print(\"\\nReshaped to (4,3):\\n\", x.reshape(4, 3))  # 4 rows, 3 cols â€” same 12 values\n",
    "print(\"\\nFlattened      :\", x.flatten())           # collapse to 1D: [1,2,...,12]\n",
    "print(\"\\nView as (2,6)  :\\n\", x.view(2, 6))        # view requires contiguous storage\n",
    "\n",
    "# â”€â”€ unsqueeze / squeeze â€” adding/removing phantom dimensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  unsqueeze: \"treat this as a batch of 1\" â€” same data, wider context\n",
    "# âš™ï¸ Used to align shapes for broadcasting or for batch-first APIs that expect (N, ...)\n",
    "y = torch.tensor([1., 2., 3.])    # shape (3,)  â€” a single vector\n",
    "print(\"\\ny.unsqueeze(0) shape:\", y.unsqueeze(0).shape)  # (1,3) â† row vector\n",
    "print(\"y.unsqueeze(1) shape:\", y.unsqueeze(1).shape)   # (3,1) â† column vector\n",
    "\n",
    "# ğŸ§  squeeze: \"stop pretending this is a batch\" â€” strip the dummy dimension\n",
    "# âš™ï¸ Removes all dims of size 1 â€” (1,3,1) â†’ (3,)\n",
    "z = torch.zeros(1, 3, 1)\n",
    "print(\"squeeze shape:\", z.squeeze().shape)             # (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5  Broadcasting\n",
    "\n",
    "Broadcasting lets tensors of different shapes work together â€” similar to NumPy.\n",
    "\n",
    "```\n",
    "Shape (3,1) + Shape (1,4) â†’ Shape (3,4)   âœ“\n",
    "Shape (3,2) + Shape (3,4) â†’ Error          âœ—\n",
    "```\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Imagine a **hormone** released by one gland (a single value or small tensor) that diffuses through the bloodstream and affects **every cell** that has the right receptor (the larger tensor). The hormone doesn't get copied for each cell â€” it's broadcast from a single source to reach all targets simultaneously.\n",
    "\n",
    "Concretely: you have a row vector `[1, 2, 3]` (one set of feature offsets) and you want to add it to a batch of 3 different samples. Instead of manually copying the row 3 times, PyTorch *broadcasts* it â€” like the same hormonal signal reaching all 3 cells at once.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Broadcasting is **implicit loop unrolling + memory aliasing**. Instead of allocating a full expanded tensor, PyTorch sets the **stride of a broadcast dimension to 0** â€” meaning the hardware reads the same memory address repeatedly, once per logical position. No extra memory is allocated.\n",
    "\n",
    "Rules (aligned from the right):\n",
    "1. If dimensions differ, prepend 1s to the smaller shape: `(3,)` â†’ `(1, 3)`\n",
    "2. If a dim is 1, it stretches to match the other: `(1, 4)` â†’ `(3, 4)`\n",
    "3. If dims are neither equal nor 1 â†’ **error**\n",
    "\n",
    "This is why `unsqueeze` is so commonly used â€” to intentionally introduce a size-1 dim so broadcasting can take over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Broadcasting in action â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  row = one hormonal signal [1, 2, 3] (one set of feature biases)\n",
    "# ğŸ§  col = three different cell types [10], [20], [30] (three samples in our batch)\n",
    "# ğŸ§  result = every cell type receives every hormonal value â†’ 3Ã—3 response grid\n",
    "\n",
    "# âš™ï¸ row shape: (1,3), col shape: (3,1)\n",
    "#    PyTorch stretches: row â†’ (3,3) with stride=0 on dim-0\n",
    "#                       col â†’ (3,3) with stride=0 on dim-1\n",
    "#    No memory allocated for the expanded tensors â€” purely stride tricks!\n",
    "row = torch.tensor([[1., 2., 3.]])          # shape (1,3)  â† one \"hormone\" pattern\n",
    "col = torch.tensor([[10.], [20.], [30.]])   # shape (3,1)  â† three \"cell types\"\n",
    "\n",
    "result = row + col                           # broadcasts to (3,3)\n",
    "print(\"Broadcast result shape:\", result.shape)\n",
    "print(result)\n",
    "# Expected:\n",
    "# [[11, 12, 13],   â† cell-type 1 (10) receives hormone [1,2,3] â†’ [11,12,13]\n",
    "#  [21, 22, 23],   â† cell-type 2 (20) receives hormone [1,2,3] â†’ [21,22,23]\n",
    "#  [31, 32, 33]]   â† cell-type 3 (30) receives hormone [1,2,3] â†’ [31,32,33]\n",
    "\n",
    "# â”€â”€ Real-world use: adding a bias vector to a batch of activations â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  In a neural layer: every sample gets the SAME bias added (broadcast over batch)\n",
    "# âš™ï¸ batch shape (32, 10) + bias shape (10,) â†’ bias broadcasts to (32, 10)\n",
    "batch      = torch.randn(32, 10)   # 32 samples, 10 features each\n",
    "bias       = torch.randn(10)       # one shared bias per feature\n",
    "out        = batch + bias          # (32,10) + (10,) â†’ broadcast â†’ (32,10)\n",
    "print(f\"\\nBatch + bias shape: {out.shape}\")   # still (32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6  Moving to GPU\n",
    "\n",
    "This is one of PyTorch's superpowers â€” seamlessly move tensors between devices.\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Think of the **CPU as your prefrontal cortex** â€” slow, deliberate, sequential thinking. It's great for complex logic, decision-making, and tasks that require one step to complete before the next can begin.\n",
    "\n",
    "Think of the **GPU as your cerebellum** â€” fast, massively parallel, handles thousands of simple operations simultaneously. The cerebellum doesn't \"think\" â€” it executes learned motor patterns at lightning speed without conscious effort.\n",
    "\n",
    "When you do matrix multiplications for deep learning (thousands of dot products per forward pass), you want the *cerebellum* doing it, not the prefrontal cortex. Moving a tensor to GPU is like handing the task off from slow conscious thought to fast automatic processing.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "- **CPU** â†’ general-purpose processor: 8â€“64 cores, each powerful, low-latency, designed for sequential tasks with branching logic\n",
    "- **GPU** â†’ massively parallel co-processor: 1,000â€“16,000 simpler cores (CUDA cores/tensor cores), optimised for throughput over latency\n",
    "- **`.to(device)`** â†’ triggers a **PCIe DMA transfer** from host RAM to device VRAM. This has latency (~1ms), so you minimise transfers by keeping data on GPU as long as possible\n",
    "- **`device=device` at creation** â†’ allocates directly in VRAM, skipping the transfer entirely\n",
    "- **`.cpu().numpy()`** â†’ transfers back to host RAM so NumPy/matplotlib can access it (they can't read GPU memory)\n",
    "\n",
    "> **Engineering rule:** Keep tensors on GPU for the entire forward + backward pass. Only move to CPU at the very end for logging/plotting. Every unnecessary transfer wastes PCIe bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Device setup â€” define once, use everywhere â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Deciding which \"part of the brain\" handles the work â€” conscious (CPU) or automatic (GPU)\n",
    "# âš™ï¸ Best practice: single device variable so you can switch hardware with one line change\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# â”€â”€ Moving an existing tensor to device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like transferring a memory from short-term (CPU RAM) to muscle memory (GPU VRAM)\n",
    "# âš™ï¸ Triggers a DMA transfer over PCIe bus â€” has latency cost, so do it once\n",
    "x = torch.randn(1000, 1000)          # allocated in CPU RAM\n",
    "x_device = x.to(device)             # move to GPU VRAM (no-op if device='cpu')\n",
    "\n",
    "print(f\"Tensor device: {x_device.device}\")\n",
    "\n",
    "# â”€â”€ Creating directly on device â€” skip the transfer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  \"Think in GPU memory from the start\" â€” no moving needed\n",
    "# âš™ï¸ Allocates in VRAM directly, bypassing PCIe â€” preferred for large tensors\n",
    "y_device = torch.randn(1000, 1000, device=device)  # born on GPU\n",
    "\n",
    "# â”€â”€ Operations happen where the tensors live â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  The cerebellum executes 1,000,000 multiply-adds in parallel â€” takes microseconds\n",
    "# âš™ï¸ GPU GEMM kernel: 1000Ã—1000 @ 1000Ã—1000 = 10^9 floating-point ops, ~1ms on modern GPU\n",
    "result = x_device @ y_device\n",
    "print(f\"Result device: {result.device}, shape: {result.shape}\")\n",
    "\n",
    "# â”€â”€ Transfer back to CPU only when needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Bring the result back to conscious awareness for inspection/plotting\n",
    "# âš™ï¸ .cpu() â†’ DMA transfer back to host RAM; .numpy() â†’ zero-copy view of that RAM\n",
    "result_cpu = result.cpu().numpy()\n",
    "print(f\"Back to NumPy: {result_cpu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7  Autograd â€” The Magic Behind Backpropagation\n",
    "\n",
    "When `requires_grad=True`, PyTorch records every operation on the tensor and can automatically compute gradients.\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "This is the most important section for understanding how neural networks *learn* â€” and it maps almost perfectly to how the biological brain learns.\n",
    "\n",
    "**Hebbian learning** (the biological version): \"Neurons that fire together, wire together.\" When you make a mistake, your brain sends a **dopamine signal** backwards through the network â€” not from input to output, but from the *result* (reward/error) back to the *cause* (which neurons fired). Synaptic weights that contributed to the error get adjusted.\n",
    "\n",
    "**Backpropagation** (PyTorch's version) is the same idea, mathematically formalised:\n",
    "1. **Forward pass** â†’ data flows forward through the network (like a thought forming)\n",
    "2. **Compute loss** â†’ measure how wrong the output was (like the brain sensing \"that was a mistake\")\n",
    "3. **`.backward()`** â†’ error signal flows *backwards* through the computation graph, computing `âˆ‚loss/âˆ‚weight` for every parameter (like dopamine signalling which synapses caused the error)\n",
    "4. **Update weights** â†’ adjust parameters in the direction that reduces error (like synaptic plasticity)\n",
    "\n",
    "PyTorch's autograd **automatically** does step 3 â€” it tracks every operation and applies the chain rule.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Autograd is a **symbolic differentiation engine** built on a dynamic computation graph (DAG):\n",
    "- **`requires_grad=True`** â†’ mark this tensor as a \"differentiable parameter\" â€” PyTorch will record every op applied to it\n",
    "- **Computation graph** â†’ a DAG where nodes are tensors and edges are operations. Built dynamically as you run code (unlike TensorFlow 1.x's static graph)\n",
    "- **`.backward()`** â†’ triggers reverse-mode automatic differentiation (reverse-mode AD). Traverses the DAG backwards, applying the chain rule at each node: `âˆ‚loss/âˆ‚x = âˆ‚loss/âˆ‚y * âˆ‚y/âˆ‚x`\n",
    "- **`.grad`** â†’ the accumulated gradient tensor (same shape as the parameter)\n",
    "- **`torch.no_grad()`** â†’ disables the DAG recorder â€” like turning off the \"tape recorder\". No graph is built, no gradients stored. Use during inference for speed + memory savings\n",
    "\n",
    "> **Key engineering principle:** The chain rule is just repeated multiplication of local derivatives. Autograd does this multiplication for you, through potentially thousands of nested operations, without you writing a single line of derivative math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Simple gradient example â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  x = a single synapse's strength (a learnable parameter)\n",
    "# ğŸ§  f = the brain's total \"cost\" for making this decision\n",
    "# ğŸ§  f.backward() = dopamine floods back: \"how much did this synapse cause the error?\"\n",
    "# âš™ï¸ PyTorch records: f = x**3 + 2*x**2 + 5 as a computation graph\n",
    "#    .backward() applies chain rule: df/dx = 3xÂ² + 4x\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)  # x=3, flagged as differentiable\n",
    "\n",
    "# f(x) = x^3 + 2x^2 + 5  â€” a simple \"loss function\" in one variable\n",
    "f = x**3 + 2*x**2 + 5\n",
    "print(f\"f(3)  = {f.item():.1f}\")           # f(3) = 27 + 18 + 5 = 50\n",
    "\n",
    "# â”€â”€ Backward pass â€” the \"dopamine signal\" flows back â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  \"Which direction should I move x to reduce f?\"\n",
    "# âš™ï¸ Traverses the computation DAG backwards; at each node applies local derivative\n",
    "f.backward()\n",
    "# df/dx at x=3: 3*(3Â²) + 4*(3) = 27 + 12 = 39\n",
    "print(f\"df/dx = {x.grad.item():.1f}   # analytical: 3xÂ²+4x = 27+12 = 39\")\n",
    "# Gradient > 0 means: increasing x increases f â€” so to DECREASE f, move x left (subtract)\n",
    "\n",
    "# â”€â”€ torch.no_grad() â€” turn off the tape recorder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like doing something on \"autopilot\" â€” you're not learning from this, just acting\n",
    "# âš™ï¸ Disables autograd DAG construction: ~30% faster, uses less memory\n",
    "#    Essential during: model inference, parameter updates (otherwise PyTorch tracks\n",
    "#    the update itself and tries to backprop through it!)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "print(f\"\\nWith no_grad, requires_grad: {y.requires_grad}\")   # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Gradient w.r.t. parameters â€” the neural net analogy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  This IS a single neuron layer:\n",
    "#    W = synaptic weights (how strongly each input neuron connects to each output neuron)\n",
    "#    b = bias (the baseline firing threshold even with zero input)\n",
    "#    x_in = the sensory signal coming in (3 input features â†’ like 3 receptor types)\n",
    "#    y_pred = the output activations (what this layer \"thinks\" â€” 2 output neurons)\n",
    "#    loss = how wrong the prediction was (the dopamine error signal)\n",
    "#\n",
    "# âš™ï¸ W shape (2,3): a linear transformation from 3D input space to 2D output space.\n",
    "#    Mathematically: y = Wx + b  â€” this IS the core of nn.Linear (minus activation fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(2, 3, requires_grad=True)   # weight matrix: 2 outputs â† 3 inputs\n",
    "b = torch.zeros(2,   requires_grad=True)    # bias: one value per output neuron\n",
    "\n",
    "x_in = torch.randn(3)                       # one sample with 3 input features\n",
    "y_pred = W @ x_in + b                       # linear layer forward pass â†’ shape (2,)\n",
    "                                             # ğŸ§  The 2 output neurons receive & combine\n",
    "                                             #    the 3 input signals, weighted by W\n",
    "\n",
    "loss   = (y_pred ** 2).sum()                # simple loss: sum of squared activations\n",
    "                                             # ğŸ§  \"How loudly are the output neurons firing?\n",
    "                                             #    We want them quiet (close to 0)\"\n",
    "\n",
    "# â”€â”€ Backward: error signal propagates back through W and b â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Dopamine signal: \"W[i,j] â€” you contributed this much to the error.\n",
    "#    Your synaptic strength should change by W.grad[i,j]\"\n",
    "# âš™ï¸ Chain rule: âˆ‚loss/âˆ‚W = âˆ‚loss/âˆ‚y_pred * âˆ‚y_pred/âˆ‚W = 2*y_pred * x_in (outer product)\n",
    "loss.backward()\n",
    "\n",
    "print(\"W.grad:\\n\", W.grad)  # shape (2,3): gradient for every synapse\n",
    "print(\"b.grad :\", b.grad)   # shape (2,):  gradient for every bias neuron\n",
    "# Gradient sign: positive â†’ increasing that weight increases loss â†’ should DECREASE it\n",
    "# Gradient magnitude: how SENSITIVE the loss is to that weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8  Real-World Mini Project: Linear Regression from Scratch\n",
    "\n",
    "**Use Case:** Predict house prices from square footage.  \n",
    "No `nn.Module` yet â€” we build everything manually to understand the training loop.\n",
    "\n",
    "**Data:** Synthetic (mimics what you'd get from a Kaggle regression dataset)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Brain Analogy â€” Learning by Experience\n",
    "Imagine a child learning to estimate room sizes. They walk into rooms, guess the size, get corrected, and gradually improve. This is *exactly* the training loop:\n",
    "\n",
    "1. **See a house** (forward pass: compute a prediction)\n",
    "2. **Estimate its price** (y_pred = W * sqft + b)\n",
    "3. **Get corrected** (\"actual price was $350k, you said $280k â€” you're off by $70k\")\n",
    "4. **Compute the error** (loss = MSE between prediction and truth)\n",
    "5. **Adjust your mental model** (backprop: compute gradients â†’ update W and b)\n",
    "6. **Repeat with the next house** (next epoch iteration)\n",
    "\n",
    "After hundreds of houses, the child's internal model (W, b) converges to something close to the true relationship. The brain has learned `price â‰ˆ 300 * sqft + 50,000`.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy â€” PID Controller / System Identification\n",
    "This training loop is mathematically identical to **gradient descent system identification**:\n",
    "- **W, b** â†’ the unknown parameters of a linear system `y = Wx + b` you're trying to identify\n",
    "- **Loss (MSE)** â†’ the squared error between your model's output and the measured system output\n",
    "- **Gradient** â†’ the direction in parameter space that most increases the error â€” so you step the *opposite* direction\n",
    "- **Learning rate `lr`** â†’ the step size; too large â†’ oscillates and diverges (unstable control); too small â†’ converges slowly\n",
    "- **Normalisation** â†’ shifting and scaling input/output to `~N(0,1)` prevents gradients from exploding or vanishing â€” equivalent to **pre-conditioning** a linear system for better numerical stability\n",
    "\n",
    "> **Why zero the gradients?** PyTorch *accumulates* gradients by default (adds to `.grad` rather than replacing). This is useful for certain advanced techniques (gradient accumulation over mini-batches), but for a standard loop you must zero them each step or you're adding stale gradients from the previous iteration â€” like a PID controller that never resets its integral term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate synthetic house price data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like a teacher preparing 200 flashcards: each card shows a house size,\n",
    "#    and the \"answer\" on the back is the true price (with a bit of real-world noise)\n",
    "# âš™ï¸ We define the ground-truth relationship: price = 300*sqft + 50000 + noise\n",
    "#    This is the \"oracle\" â€” what our model is trying to learn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# True relationship: price = 300 * sqft + 50000 + noise\n",
    "sqft  = torch.linspace(500, 3500, 200).reshape(-1, 1)   # shape (200,1): 200 houses\n",
    "noise = torch.randn(200, 1) * 20000                      # real-world noise (~$20k std)\n",
    "price = 300 * sqft + 50000 + noise                       # shape (200,1): true prices\n",
    "\n",
    "# â”€â”€ Normalise â€” pre-condition the system for stable gradients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like converting everything to a common scale before comparing:\n",
    "#    \"sqft\" ranges from 500â€“3500; \"price\" ranges from 200kâ€“1.1M\n",
    "#    Hard to learn across such different scales â€” normalise so both are in [-2, +2]\n",
    "# âš™ï¸ Z-score normalisation: x_norm = (x - mean) / std\n",
    "#    â†’ both inputs and outputs have meanâ‰ˆ0, stdâ‰ˆ1\n",
    "#    â†’ gradients are well-conditioned (similar magnitudes), converges faster\n",
    "sqft_norm  = (sqft  - sqft.mean())  / sqft.std()\n",
    "price_norm = (price - price.mean()) / price.std()\n",
    "\n",
    "# â”€â”€ Visualise the raw data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Like the teacher showing you all 200 flashcards at once â€” see the pattern?\n",
    "# âš™ï¸ Each dot = one training example (sqft, price). The signal-to-noise ratio\n",
    "#    is high here, so linear regression should work well.\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(sqft.numpy(), price.numpy(), alpha=0.5, s=20)\n",
    "plt.xlabel(\"Square Footage\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.title(\"Synthetic House Price Data (True: price = 300Â·sqft + 50k + noise)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Linear Regression Training Loop (pure tensor math) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# ğŸ§  THE LEARNING LOOP â€” 5 steps that mirror how the brain learns:\n",
    "#   Step 1: PERCEIVE   (forward pass â€” form a prediction)\n",
    "#   Step 2: EVALUATE   (compute loss â€” how wrong was that prediction?)\n",
    "#   Step 3: REFLECT    (backward pass â€” which weights caused the error?)\n",
    "#   Step 4: ADJUST     (gradient descent â€” update weights to reduce error)\n",
    "#   Step 5: RESET      (zero gradients â€” clear the \"working memory\" for next round)\n",
    "#\n",
    "# âš™ï¸ This loop implements Gradient Descent optimisation:\n",
    "#   Î¸ â† Î¸ - lr * âˆ‚L/âˆ‚Î¸    (where Î¸ = parameters, L = loss, lr = step size)\n",
    "#   It's equivalent to the Widrow-Hoff LMS (Least Mean Squares) update rule\n",
    "#   used in adaptive filters and signal processing.\n",
    "\n",
    "W = torch.randn(1, 1, requires_grad=True)   # weight: guess of slope     (initially random)\n",
    "b = torch.zeros(1,    requires_grad=True)   # bias:   guess of intercept (initially 0)\n",
    "\n",
    "lr     = 0.01   # step size â€” like the \"learning rate\" of a student (too high â†’ panic, too low â†’ slow)\n",
    "epochs = 200    # number of times we show ALL 200 houses to the model\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # â”€â”€ Step 1: FORWARD PASS â€” form a prediction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ğŸ§  \"Given this house size, what price do I think it is?\" (WÂ·sqft + b)\n",
    "    # âš™ï¸ Matrix multiply: (200,1) @ (1,1) + (1,) â†’ (200,1) â€” one prediction per house\n",
    "    y_pred = sqft_norm @ W + b\n",
    "\n",
    "    # â”€â”€ Step 2: COMPUTE LOSS â€” measure the error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ğŸ§  \"How far off was I? Across all 200 houses?\"\n",
    "    # âš™ï¸ MSE = mean( (Å· - y)Â² ) â€” the squared error penalises large mistakes more\n",
    "    loss = ((y_pred - price_norm) ** 2).mean()\n",
    "\n",
    "    # â”€â”€ Step 3: BACKWARD PASS â€” compute gradients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ğŸ§  \"Which way should W and b move to reduce my mistakes?\"\n",
    "    # âš™ï¸ Autograd traverses DAG backwards; populates W.grad and b.grad\n",
    "    loss.backward()\n",
    "\n",
    "    # â”€â”€ Step 4: UPDATE PARAMETERS â€” gradient descent step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ğŸ§  \"Adjust my mental model slightly in the right direction\"\n",
    "    # âš™ï¸ MUST use no_grad: we don't want PyTorch to track the update itself\n",
    "    #    (otherwise it would try to backprop through the update â€” circular!)\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad    # move W opposite to gradient (downhill on loss surface)\n",
    "        b -= lr * b.grad    # same for bias\n",
    "\n",
    "    # â”€â”€ Step 5: ZERO GRADIENTS â€” clear working memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ğŸ§  \"Forget the gradient from this round â€” don't let it contaminate the next\"\n",
    "    # âš™ï¸ PyTorch accumulates grads by default (.grad += new_grad each backward())\n",
    "    #    We MUST zero before the next forward pass, or gradients compound incorrectly\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Learned W: {W.item():.4f}, b: {b.item():.4f}\")\n",
    "print(f\"(In normalised space â€” both should be close to the true normalised slope & intercept)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualise training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§  Left plot: the \"learning curve\" â€” like charting a student's improvement over time.\n",
    "#    A healthy brain shows rapid early improvement (steep drop) then plateaus.\n",
    "#    If it never drops â†’ the model isn't learning (lr too small or wrong architecture).\n",
    "#    If it drops then spikes â†’ the model is unstable (lr too large).\n",
    "#\n",
    "# ğŸ§  Right plot: the \"fitted model\" â€” has the brain internalized the true relationship?\n",
    "#    Red line = the model's belief about how sqft â†’ price after 200 training rounds.\n",
    "#    Blue dots = the ground truth examples it learned from.\n",
    "#\n",
    "# âš™ï¸ Left: Loss curve should show exponential-ish decay â†’ log-linear in log-space.\n",
    "#    This is characteristic of gradient descent on a convex loss surface (MSE is convex!).\n",
    "# âš™ï¸ Right: A straight-line fit on normalised data. W â‰ˆ 1.0 and b â‰ˆ 0.0 means\n",
    "#    the model has recovered the true linear relationship in normalised space.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# â”€â”€ Loss curve (learning progress) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax1.plot(losses, color='steelblue')\n",
    "ax1.set_xlabel(\"Epoch (training round)\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax1.set_title(\"Training Loss Curve\\nğŸ§  Student improving | âš™ï¸ Gradient descent on convex surface\")\n",
    "ax1.axhline(y=losses[-1], color='red', linestyle='--', alpha=0.5, label=f'Final: {losses[-1]:.4f}')\n",
    "ax1.legend()\n",
    "\n",
    "# â”€â”€ Regression fit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with torch.no_grad():          # inference only â€” no need to track gradients\n",
    "    pred = sqft_norm @ W + b   # apply learned W, b to all 200 normalised inputs\n",
    "\n",
    "ax2.scatter(sqft_norm.numpy(), price_norm.numpy(), alpha=0.4, s=20, label=\"Training data\")\n",
    "ax2.plot(sqft_norm.numpy(), pred.numpy(), color='red', lw=2,\n",
    "         label=f\"Learned fit  W={W.item():.3f}, b={b.item():.3f}\")\n",
    "ax2.set_xlabel(\"sqft (normalised)\")\n",
    "ax2.set_ylabel(\"price (normalised)\")\n",
    "ax2.set_title(\"Linear Regression Fit\\nğŸ§  Learned belief | âš™ï¸ Identified system parameters\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  Learned W={W.item():.4f} in normalised space\")\n",
    "print(f\"  Since data is z-scored, Wâ‰ˆ1.0 confirms the model learned the dominant linear trend.\")\n",
    "print(f\"  bâ‰ˆ0.0 confirms the normalised intercept (mean is already removed by z-scoring).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9  Quick Reference â€” Tensor Cheatsheet\n",
    "\n",
    "| Operation | Code | Notes |\n",
    "|-----------|------|-------|\n",
    "| Create | `torch.tensor(data)` | from Python list/ndarray |\n",
    "| Zeros | `torch.zeros(r, c)` | |\n",
    "| Random | `torch.randn(r, c)` | N(0,1) |\n",
    "| Shape | `x.shape` or `x.size()` | |\n",
    "| Reshape | `x.reshape(a, b)` | |\n",
    "| Flatten | `x.flatten()` | |\n",
    "| To device | `x.to(device)` | |\n",
    "| To numpy | `x.cpu().numpy()` | |\n",
    "| MatMul | `a @ b` | |\n",
    "| Grad | `x.backward()` | |\n",
    "| No grad | `torch.no_grad()` | inference only |\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Create a tensor of shape `(5, 5)` with random values and find the row with the highest mean.\n",
    "2. Implement the gradient of `f(x) = sin(xÂ²)` at `x = Ï€/4` using autograd. Verify with calculus.\n",
    "3. Modify the house price model to use **batch gradient descent** (update every N samples).\n",
    "\n",
    "---\n",
    "**Next â†’** [Module 02: Your First Neural Network with nn.Module](./Module_02_First_Neural_Network.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}