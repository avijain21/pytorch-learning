{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Transfer Learning ‚Äî Fine-Tuning Pretrained Models\n",
    "\n",
    "---\n",
    "## Don't Start from Scratch\n",
    "\n",
    "### üß† Brain Analogy\n",
    "A radiologist spends 6 years in medical school learning general medicine, then 2 years specialising in reading X-rays. They don't re-learn anatomy ‚Äî they **transfer** general knowledge to the new speciality.\n",
    "\n",
    "Transfer learning works the same way:\n",
    "- **ImageNet pretraining** = medical school (1.2M images, 1000 classes, months of GPU time)\n",
    "- **Your fine-tuning** = specialisation (hundreds of your images, minutes to hours)\n",
    "- **Pretrained weights** = accumulated expertise: edges ‚Üí shapes ‚Üí textures ‚Üí object parts\n",
    "\n",
    "The model already knows how to see. You just teach it YOUR specific patterns.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "ResNet-18 trained on ImageNet = a visual feature hierarchy producing rich 512-d embeddings for any image. Transfer learning: remove old head (1000 classes) ‚Üí add new head (your N classes). Two strategies: Feature Extraction (freeze backbone, fast) or Fine-Tuning (partial unfreeze, more accurate).\n",
    "\n",
    "**Level:** Intermediate  \n",
    "**Duration:** ~3 hours  \n",
    "**Dataset:** Dogs vs Cats ([Kaggle](https://www.kaggle.com/c/dogs-vs-cats)) + built-in torchvision subsets  \n",
    "**Real-World Use Case:** Medical imaging, defect detection, any domain with limited labeled data\n",
    "\n",
    "## What You'll Learn\n",
    "- Loading pretrained ImageNet models from `torchvision.models`\n",
    "- Feature extraction (freeze backbone, train only head)\n",
    "- Fine-tuning (unfreeze layers progressively)\n",
    "- Differential learning rates (different LR for each layer group)\n",
    "- Grad-CAM ‚Äî visualising what the model focuses on\n",
    "\n",
    "## Why Transfer Learning?\n",
    "```\n",
    "Training ResNet-50 from scratch on ImageNet:\n",
    "  ‚Üí 1.2M images, ~90 GPU hours\n",
    "\n",
    "Transfer Learning to your 1000-image dataset:\n",
    "  ‚Üí ~10 minutes, ~90% accuracy  ‚úì\n",
    "\n",
    "Pretrained weights = millions of training hours of feature learning ‚Äî already done for you!\n",
    "```\n",
    "\n",
    "## Two Strategies\n",
    "```\n",
    "1. Feature Extraction:  freeze ALL backbone layers ‚Üí only train new head\n",
    "   Best when: your data is small & similar to ImageNet\n",
    "\n",
    "2. Fine-Tuning:         unfreeze last N layers ‚Üí train with small LR\n",
    "   Best when: your data is larger or quite different from ImageNet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Loading tools: torchvision has pretrained models ready to download\n",
    "# ‚öôÔ∏è torchvision.models provides ResNet/EfficientNet/ViT with pretrained ImageNet weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1  Dataset Options\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Pretrained models were trained with specific colour settings. Using different settings = giving a doctor inverted X-rays ‚Äî their pattern matching breaks completely. ALWAYS use the same normalisation (mean/std) the model was trained with.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "ALL pretrained models require ImageNet normalisation: `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`. Wrong normalisation silently breaks pretrained features ‚Äî model outputs garbage. Val: `CenterCrop(224)` (deterministic). Train: `RandomCrop(224)` (free augmentation).\n",
    "\n",
    "### Option A ‚Äî Kaggle Dogs vs Cats (recommended for full experience)\n",
    "```bash\n",
    "# Install Kaggle CLI and download:\n",
    "pip install kaggle\n",
    "kaggle competitions download -c dogs-vs-cats\n",
    "unzip dogs-vs-cats.zip\n",
    "# Then organize into data/dogs_cats/train/dogs/ and data/dogs_cats/train/cats/\n",
    "```\n",
    "\n",
    "### Option B ‚Äî Use STL-10 (built-in, no account needed)\n",
    "We use STL-10 below (96√ó96 images, similar task ‚Äî animals vs objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Same \"colour calibration\" as original training ‚Äî wrong calibration breaks all the learned patterns\n",
    "# ‚öôÔ∏è Resize(256)‚ÜíCenterCrop(224): standard val preprocessing for all 224√ó224 pretrained models\n",
    "# ‚îÄ‚îÄ ImageNet normalization stats (required for pretrained models) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ‚îÄ‚îÄ Transforms ‚Äî resize to 224√ó224 (ImageNet standard) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# ‚îÄ‚îÄ Using STL-10 (10 classes, 96√ó96, 5000 train + 8000 test) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "stl_train = torchvision.datasets.STL10('./data', split='train',\n",
    "                                        transform=train_transform, download=True)\n",
    "stl_test  = torchvision.datasets.STL10('./data', split='test',\n",
    "                                        transform=val_transform, download=True)\n",
    "\n",
    "# Use only 2 classes for binary classification demo (airplane=0, bird=1)\n",
    "def filter_classes(dataset, classes):\n",
    "    indices = [i for i, (_, lbl) in enumerate(dataset) if lbl in classes]\n",
    "    dataset = torch.utils.data.Subset(dataset, indices)\n",
    "    return dataset\n",
    "\n",
    "# airplane=0, bird=1 in STL-10\n",
    "train_ds = filter_classes(stl_train, [0, 1])\n",
    "test_ds  = filter_classes(stl_test,  [0, 1])\n",
    "\n",
    "# Split train into train+val\n",
    "n_train = int(0.8 * len(train_ds))\n",
    "n_val   = len(train_ds) - n_train\n",
    "train_ds, val_ds = random_split(train_ds, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "CLASS_NAMES = ['airplane', 'bird']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  Load Pretrained ResNet-18\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Loading an expert who has already seen 1.2 million images. Their brain can classify 1000 objects. We just replace their \"final decision layer\" ‚Äî currently mapping 512 features to 1000 ImageNet classes ‚Äî with one for our 2 classes. Keep all learned visual knowledge, swap the final decision.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "ResNet-18: `layer4 ‚Üí avgpool ‚Üí fc(512‚Üí1000)`. Replace `fc(512‚Üí1000)` with custom head. `in_features = model.fc.in_features = 512` ‚Äî always the bottleneck size regardless of what head is attached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Load the expert: 1.2M images studied, 1000 objects known, ready to specialise\n",
    "# ‚öôÔ∏è weights=DEFAULT downloads pretrained checkpoint (~45MB from PyTorch Hub)\n",
    "# ‚îÄ‚îÄ Available pretrained models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# models.resnet18, resnet50, resnet101\n",
    "# models.vgg16, vgg19\n",
    "# models.efficientnet_b0, efficientnet_b4\n",
    "# models.mobilenet_v3_small, mobilenet_v3_large\n",
    "# models.vit_b_16  (Vision Transformer)\n",
    "\n",
    "backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Inspect the architecture\n",
    "print(backbone)\n",
    "print(f\"\\nOriginal FC layer: {backbone.fc}\")\n",
    "print(f\"Original output classes: {backbone.fc.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3  Strategy 1: Feature Extraction (Frozen Backbone)\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Strategy 1: Lock the expert's general knowledge, teach only the new speciality decision. Freeze all backbone parameters ‚Äî they hold valuable learned visual features. Only train the new head. Works because ImageNet features are general enough to describe airplanes and birds too.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "`requires_grad=False` ‚Üí autograd skips frozen params. `filter(p.requires_grad, params)` ‚Üí only trainable params to optimizer. Only ~2% of parameters train ‚Üí very fast, minimal overfitting risk with small datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Lock the expert's knowledge, only teach the new 2-class decision layer\n",
    "# ‚öôÔ∏è requires_grad=False prevents gradient computation; filter ensures optimizer only tracks trainable params\n",
    "def build_feature_extractor(num_classes=2):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    # ‚ë† Freeze ALL backbone parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # ‚ë° Replace final fully connected layer (this is the only trainable part)\n",
    "    in_features = model.fc.in_features   # 512 for ResNet-18\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    # The new fc layer has requires_grad=True by default\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "fe_model = build_feature_extractor(num_classes=2).to(device)\n",
    "\n",
    "# Only new head parameters are trained\n",
    "trainable = sum(p.numel() for p in fe_model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in fe_model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Training only the speciality decision ‚Äî backbone just extracts features unchanged\n",
    "# ‚öôÔ∏è filter(requires_grad) in optimizer: saves memory and state vs passing all parameters\n",
    "# ‚îÄ‚îÄ Train feature extractor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, model_name):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Only pass trainable parameters to optimizer\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    history = {'tr_acc': [], 'vl_acc': []}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds   = model(X).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += len(y)\n",
    "        tr_acc = correct / total\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds   = model(X).argmax(1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total   += len(y)\n",
    "        vl_acc = correct / total\n",
    "\n",
    "        scheduler.step()\n",
    "        history['tr_acc'].append(tr_acc)\n",
    "        history['vl_acc'].append(vl_acc)\n",
    "\n",
    "        if vl_acc > best_acc:\n",
    "            best_acc = vl_acc\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            print(f\"{model_name} | Epoch {epoch:2d}/{epochs} | \"\n",
    "                  f\"train {tr_acc:.3f} | val {vl_acc:.3f}\")\n",
    "\n",
    "    print(f\"\\nBest val acc ({model_name}): {best_acc:.1%}\\n\")\n",
    "    return history, best_acc\n",
    "\n",
    "\n",
    "fe_history, fe_best = train_model(fe_model, train_loader, val_loader,\n",
    "                                   epochs=15, lr=1e-3, model_name='fe_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4  Strategy 2: Fine-Tuning (Unfreeze Last Layers)\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Strategy 2: Allow the expert to also refine their high-level perception for the new task. Unfreeze `layer4` ‚Äî it detects high-level features (wings/fuselage for planes, feathers/beak for birds) that ARE task-specific. Keep early layers frozen ‚Äî they detect universal features (edges, textures) shared across all visual tasks.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "`if 'layer4' not in name and 'fc' not in name: requires_grad = False`. Layer1-3: universal visual primitives (freeze). Layer4: high-level, task-specific (unfreeze). FC: new head (always trainable). Trainable params increase from 2% to ~15%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Unlock high-level vision (layer4) while keeping basic vision (edges, textures) frozen\n",
    "# ‚öôÔ∏è name-based selection: \"layer4\" in name targets only the final convolutional block\n",
    "def build_finetuned_model(num_classes=2):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    # Freeze early layers (they detect generic features like edges)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'layer4' not in name and 'fc' not in name:\n",
    "            param.requires_grad = False    # freeze layer1-3\n",
    "\n",
    "    # Replace head\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "ft_model = build_finetuned_model().to(device)\n",
    "trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in ft_model.parameters())\n",
    "print(f\"Fine-tune trainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Differential learning rates ‚Äî critical for fine-tuning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Differential learning rates = \"fast lane for new speciality, slow lane for existing knowledge.\" Update backbone with 10√ó smaller LR = tiny adjustments that gently tune, not overwrite. Without this, fine-tuning can cause catastrophic forgetting of ImageNet features.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "Two param groups with different `lr`: `backbone_params: lr=1e-4`, `head_params: lr=1e-3`. This is standard practice for all production fine-tuning. Without differential LR, fine-tuning often hurts performance on small datasets.\n",
    "\n",
    "# Head gets 10√ó higher learning rate than backbone\n",
    "\n",
    "backbone_params = [p for n, p in ft_model.named_parameters()\n",
    "                   if 'fc' not in n and p.requires_grad]\n",
    "head_params     = [p for n, p in ft_model.named_parameters() if 'fc' in n]\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': 1e-4},   # lower LR for pretrained layers\n",
    "    {'params': head_params,     'lr': 1e-3},   # higher LR for new head\n",
    "])\n",
    "\n",
    "print(\"Optimizer with differential learning rates:\")\n",
    "for i, pg in enumerate(optimizer.param_groups):\n",
    "    n = sum(p.numel() for p in pg['params'])\n",
    "    print(f\"  Group {i}: lr={pg['lr']}, params={n:,}\")\n",
    "\n",
    "ft_history, ft_best = train_model(ft_model, train_loader, val_loader,\n",
    "                                   epochs=15, lr=1e-3, model_name='ft_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5  Compare Feature Extraction vs Fine-Tuning\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Side-by-side: \"Did refreshing high-level vision help vs just teaching the new decision?\" Fine-tuning typically gains 2-5% when the target domain differs enough from ImageNet to benefit from backbone adaptation.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "FE = Feature Extraction (frozen backbone, fast, low overfitting risk). FT = Fine-Tuning (layer4+head trainable, higher accuracy, needs more data to avoid overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Which worked better: locked expert vs. refreshed expert? Side-by-side comparison.\n",
    "# ‚öôÔ∏è FE = Feature Extraction; FT = Fine-Tuning. Bar chart shows best val accuracy per strategy.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(fe_history['tr_acc'], '‚Äìo', label='FE Train',  color='blue',   alpha=0.7)\n",
    "ax1.plot(fe_history['vl_acc'], '‚Äìs', label='FE Val',    color='blue',   alpha=0.4)\n",
    "ax1.plot(ft_history['tr_acc'], '‚Äìo', label='FT Train',  color='orange', alpha=0.7)\n",
    "ax1.plot(ft_history['vl_acc'], '‚Äìs', label='FT Val',    color='orange', alpha=0.4)\n",
    "ax1.set_title('Accuracy: Feature Extraction vs Fine-Tuning')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "bars = ax2.bar(['Feature\\nExtraction', 'Fine-Tuning'],\n",
    "               [fe_best, ft_best],\n",
    "               color=['steelblue', 'darkorange'], width=0.4)\n",
    "for bar, val in zip(bars, [fe_best, ft_best]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{val:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.set_title('Best Validation Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "plt.suptitle('Transfer Learning Strategies Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6  Grad-CAM ‚Äî What Does the Model See?\n",
    "\n",
    "### üß† Brain Analogy\n",
    "If the model says \"airplane\" ‚Äî WHERE in the image was it looking? Grad-CAM highlights regions that influenced the decision. A good model focuses on the OBJECT (wings, fuselage), not the background (sky). Highlighting background = \"spurious correlation\" ‚Äî will fail on airplanes on runways.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "Grad-CAM: gradient of class score w.r.t. target conv layer ‚Üí spatial importance weights ‚Üí weighted feature map sum ‚Üí ReLU ‚Üí resize. Works post-hoc on ANY pretrained model without modification. Standard tool for model interpretability.\n",
    "\n",
    "Gradient-weighted Class Activation Mapping shows which image regions influenced the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† \"Where was the model looking when it decided airplane vs bird?\" ‚Äî model attention map\n",
    "# ‚öôÔ∏è Grad-CAM: backward hook captures dScore/dActivation; weight feature maps by importance\n",
    "class GradCAM:\n",
    "    \"\"\"Simplified Grad-CAM for any model with a target conv layer.\"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.gradients   = None\n",
    "        self.activations = None\n",
    "\n",
    "        target_layer.register_forward_hook(\n",
    "            lambda m, i, o: setattr(self, 'activations', o)\n",
    "        )\n",
    "        target_layer.register_full_backward_hook(\n",
    "            lambda m, gi, go: setattr(self, 'gradients', go[0])\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        self.model.eval()\n",
    "        logits = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = logits.argmax(1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        logits[0, class_idx].backward()\n",
    "\n",
    "        # Pool gradients over spatial dims ‚Üí importance per channel\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)   # (1,C,1,1)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)  # (1,1,H,W)\n",
    "        cam = torch.relu(cam)                                         # only positives\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        return cam.squeeze().cpu().numpy(), class_idx\n",
    "\n",
    "\n",
    "# Attach Grad-CAM to layer4 of ResNet (last conv block)\n",
    "ft_model.load_state_dict(torch.load('ft_model_best.pth', map_location=device))\n",
    "grad_cam = GradCAM(ft_model, ft_model.layer4[1].conv2)\n",
    "\n",
    "# Run on a few validation images\n",
    "def unnormalize(t):\n",
    "    t = t.clone()\n",
    "    for c, (m, s) in enumerate(zip(IMAGENET_MEAN, IMAGENET_STD)):\n",
    "        t[c] = t[c] * s + m\n",
    "    return t.permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "\n",
    "images, labels = next(iter(DataLoader(val_ds, batch_size=6, shuffle=True)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(16, 6))\n",
    "for i in range(6):\n",
    "    img_t = images[i:i+1].to(device).requires_grad_(True)\n",
    "    cam, pred = grad_cam(img_t)\n",
    "\n",
    "    original = unnormalize(images[i])\n",
    "    import cv2\n",
    "    heatmap = plt.get_cmap('jet')(cam)[:, :, :3]   # no alpha\n",
    "\n",
    "    axes[0][i].imshow(original)\n",
    "    axes[0][i].set_title(f\"True:{CLASS_NAMES[labels[i].item()]}\\nPred:{CLASS_NAMES[pred]}\",\n",
    "                          fontsize=8)\n",
    "    axes[0][i].axis('off')\n",
    "\n",
    "    # Overlay (resize CAM to image size)\n",
    "    import PIL.Image\n",
    "    cam_resized = np.array(PIL.Image.fromarray((cam * 255).astype(np.uint8)).resize(\n",
    "        (original.shape[1], original.shape[0]), PIL.Image.BILINEAR\n",
    "    )) / 255.0\n",
    "    overlay = 0.5 * original + 0.5 * plt.get_cmap('jet')(cam_resized)[:, :, :3]\n",
    "\n",
    "    axes[1][i].imshow(np.clip(overlay, 0, 1))\n",
    "    axes[1][i].set_title(\"Grad-CAM\", fontsize=8)\n",
    "    axes[1][i].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM: Regions Used for Prediction', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7  Quick Reference ‚Äî Pretrained Model APIs\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Quick guide to different pretrained \"expert brain\" models. Each was trained differently: ResNet uses skip connections, EfficientNet scales depth/width together, ViT uses attention instead of convolution.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "All models expose `in_features` for head input dimension ‚Äî never hardcode 512 as it varies by architecture. ONNX export enables framework-independent deployment for production systems.\n",
    "\n",
    "```python\n",
    "# Available models (torchvision 0.15+)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)   # Vision Transformer\n",
    "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "# Get head input size programmatically\n",
    "in_features = model.fc.in_features           # ResNet, InceptionNet\n",
    "in_features = model.classifier[-1].in_features  # EfficientNet, MobileNet\n",
    "in_features = model.heads.head.in_features   # ViT\n",
    "\n",
    "# Replace head\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "```\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Download the [Dogs vs Cats dataset from Kaggle](https://www.kaggle.com/c/dogs-vs-cats) and use `ImageFolder`.\n",
    "2. Swap ResNet-18 for EfficientNet-B0. Compare accuracy and inference time.\n",
    "3. Implement **progressive unfreezing**: start with only the head, then unfreeze layer4, then layer3, etc. every 5 epochs.\n",
    "4. Export the model to **ONNX** for deployment: `torch.onnx.export(model, dummy, 'model.onnx')`.\n",
    "\n",
    "---\n",
    "**Next ‚Üí** [Module 06: RNNs & LSTMs for Sequential Data](./Module_06_RNN_LSTM_Sentiment.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}