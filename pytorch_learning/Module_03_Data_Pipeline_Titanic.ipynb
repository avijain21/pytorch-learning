{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Data Pipelines ‚Äî Dataset, DataLoader & Transforms\n",
    "\n",
    "---\n",
    "## Why Data Pipelines Matter\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Before your brain can learn, info needs to arrive in the right format and pace. A PyTorch data pipeline is your study assistant: **Dataset** = the whole flashcard collection (891 passengers), **DataLoader** = hands you 32 cards at a time shuffled, **Feature Engineering** = translating raw text (\"male\") into numbers.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "80% of real ML work is data wrangling. PyTorch separates data logic (Dataset) from model logic (nn.Module). Clean pipelines are reproducible, testable, reusable.\n",
    "\n",
    "**Level:** Beginner ‚Üí Intermediate  \n",
    "**Duration:** ~3 hours  \n",
    "**Dataset:** Titanic Survival ([Kaggle Competition](https://www.kaggle.com/competitions/titanic/data))  \n",
    "**Real-World Use Case:** Tabular binary classification with feature engineering\n",
    "\n",
    "## What You'll Learn\n",
    "- Custom `torch.utils.data.Dataset` class\n",
    "- `DataLoader` ‚Äî batching, shuffling, parallel loading\n",
    "- Feature engineering & preprocessing pipeline\n",
    "- Handling class imbalance (weighted sampling)\n",
    "- Validation split strategies\n",
    "- Experiment tracking with a training history dict\n",
    "\n",
    "## Why This Matters\n",
    "In real projects 80% of the work is data wrangling. A clean PyTorch data pipeline separates your **data logic** from **model logic** ‚Äî making code maintainable and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Gathering all tools ‚Äî data loading, preprocessing, evaluation instruments\n",
    "# ‚öôÔ∏è WeightedRandomSampler = fix class imbalance | roc_auc_score = better than accuracy for skewed data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Download & Load Titanic Data\n",
    "\n",
    "### üß† Brain Analogy\n",
    "891 passengers, each described by age, sex, class, family size, fare. The brain must learn: who survived? Like a detective inferring outcomes from evidence ‚Äî \"women in 1st class had priority access to lifeboats.\"\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "Binary classification: `y ‚àà {0,1}`. Only 38% survived ‚Üí class imbalance. Missing values need imputation. AUC-ROC is better than accuracy for imbalanced data.\n",
    "\n",
    "```bash\n",
    "# Option A ‚Äî Kaggle API:\n",
    "kaggle competitions download -c titanic\n",
    "\n",
    "# Option B ‚Äî Direct URL (no auth required):\n",
    "# We use seaborn's built-in version below\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Opening the historical record of 891 passengers ‚Äî 38% survived\n",
    "# ‚öôÔ∏è 38% survival = class imbalance; naive \"always guess died\" gets 62% accuracy for free!\n",
    "# Using seaborn's built-in Titanic (same data, no Kaggle account needed)\n",
    "df = sns.load_dataset('titanic')\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(\"\\nSurvival rate:\", df['survived'].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Detectives review evidence before forming a theory ‚Äî look for survival patterns\n",
    "# ‚öôÔ∏è EDA: spot class imbalance and which features (sex, class, age) predict survival\n",
    "# ‚îÄ‚îÄ Exploratory plots ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "sns.barplot(data=df, x='sex',    y='survived', ax=axes[0]).set_title('Survival by Sex')\n",
    "sns.barplot(data=df, x='pclass', y='survived', ax=axes[1]).set_title('Survival by Class')\n",
    "df[df['age'].notna()].groupby('survived')['age'].plot.hist(\n",
    "    ax=axes[2], alpha=0.6, bins=25, legend=True\n",
    ")\n",
    "axes[2].set_title('Age Distribution by Survival')\n",
    "axes[2].legend(['Died (0)', 'Survived (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Feature Engineering\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Raw data is in a foreign language ‚Äî translate it: \"male\"‚Üí1, create new clues (\"travelling alone?\", \"fare per person\"). Domain knowledge matters: families tried to stay together ‚Üí create `family_size`.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "Feature engineering: `x_raw ‚àà mixed_types` ‚Üí `x_clean ‚àà ‚Ñù¬π¬π`. Impute NaN with median/mode. Encode categoricals. Create interaction terms (`age √ó pclass`). Derived features inject domain knowledge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Translate foreign-language data into 11 numbers the brain can process per passenger\n",
    "# ‚öôÔ∏è Systematic pipeline: impute ‚Üí encode ‚Üí engineer ‚Üí select feature columns\n",
    "def feature_engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build a clean feature matrix from raw Titanic data.\"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    # ‚îÄ‚îÄ Fill missing values ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    d['age']      = d['age'].fillna(d['age'].median())\n",
    "    d['embarked'] = d['embarked'].fillna(d['embarked'].mode()[0])\n",
    "    d['fare']     = d['fare'].fillna(d['fare'].median())\n",
    "\n",
    "    # ‚îÄ‚îÄ Encode categoricals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    d['sex_enc']  = (d['sex'] == 'male').astype(int)   # 0=female, 1=male\n",
    "    d['emb_enc']  = LabelEncoder().fit_transform(d['embarked'])\n",
    "\n",
    "    # ‚îÄ‚îÄ Engineered features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    d['family_size'] = d['sibsp'] + d['parch'] + 1\n",
    "    d['is_alone']    = (d['family_size'] == 1).astype(int)\n",
    "    d['fare_per_person'] = d['fare'] / d['family_size']\n",
    "    d['age_class']   = d['age'] * d['pclass']          # interaction term\n",
    "\n",
    "    feature_cols = [\n",
    "        'pclass', 'sex_enc', 'age', 'sibsp', 'parch', 'fare',\n",
    "        'emb_enc', 'family_size', 'is_alone', 'fare_per_person', 'age_class'\n",
    "    ]\n",
    "    return d[feature_cols], d['survived']\n",
    "\n",
    "\n",
    "X, y = feature_engineer(df)\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "X.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Custom PyTorch Dataset Class\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Dataset = a library of memories. `__init__` organises the shelf. `__len__` says how many \"books\" exist. `__getitem__(i)` hands you book number i. DataLoader is the librarian ‚Äî it picks books and delivers them in batches.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "`Dataset` protocol: `__len__` enables `len()`, `__getitem__` enables indexing. Convert to tensors in `__init__` once (not per sample = faster). `y.reshape(-1,1)` required for `BCEWithLogitsLoss` expecting shape (N,1).\n",
    "\n",
    "The three mandatory methods: `__init__`, `__len__`, `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Library of passenger flashcards ‚Äî 11 features + survived/died label per card\n",
    "# ‚öôÔ∏è Convert to tensors ONCE in __init__ (amortises conversion cost across all samples)\n",
    "class TitanicDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset wrapping the Titanic tabular data.\"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        # Store as tensors\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).reshape(-1, 1)   # shape (N,1) for BCE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Split data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X_np = X.values.astype(np.float32)\n",
    "y_np = y.values.astype(np.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ Normalize ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "\n",
    "train_ds = TitanicDataset(X_train, y_train)\n",
    "val_ds   = TitanicDataset(X_val,   y_val)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "x_sample, y_sample = train_ds[0]\n",
    "print(f\"Sample features shape: {x_sample.shape}, label: {y_sample.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4  DataLoader ‚Äî Batching, Shuffling, Weighted Sampling\n",
    "\n",
    "### üß† Brain Analogy\n",
    "DataLoader = study assistant: shuffles cards, hands you 32 at a time. Class imbalance fix: 620 \"died\" vs 270 \"survived\" ‚Üí brain learns \"just guess died\" (lazy!). WeightedRandomSampler gives survivors 2√ó draw probability ‚Üí balanced study sessions.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "`WeightedRandomSampler`: per-sample prob = `1/class_count`. Corrects gradient bias toward majority class. `pin_memory=True`: pre-locks CPU memory ‚Üí faster PCIe DMA to GPU. Cannot use both sampler AND shuffle simultaneously.\n",
    "\n",
    "The Titanic dataset is **imbalanced** (~38% survived). We handle this with weighted sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Balance the study deck: give rare survivors same frequency as common deaths\n",
    "# ‚öôÔ∏è sample_weight[i] = 1/class_count[class_i] ‚Üí equalises effective class distribution per batch\n",
    "# ‚îÄ‚îÄ Handle class imbalance with WeightedRandomSampler ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class_counts = np.bincount(y_train.astype(int))   # [n_died, n_survived]\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = class_weights[y_train.astype(int)]   # weight per sample\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(train_ds),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    sampler=sampler,      # replaces shuffle=True\n",
    "    num_workers=0,        # increase for large datasets\n",
    "    pin_memory=(device.type == 'cuda')\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Class counts: died={class_counts[0]}, survived={class_counts[1]}\")\n",
    "print(f\"Class weights: died={class_weights[0]:.3f}, survived={class_weights[1]:.3f}\")\n",
    "\n",
    "# Inspect a batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes ‚Üí X: {xb.shape}, y: {yb.shape}\")\n",
    "print(f\"Batch survival rate: {yb.mean().item():.2%}  (should be ‚âà50% now)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5  Model ‚Äî Deep Tabular Network\n",
    "\n",
    "### üß† Brain Analogy\n",
    "BatchNorm normalises internal brain signals between layers ‚Äî keeps activations in a comfortable range. Dropout 0.3 is stronger regularisation for noisy Titanic data. BCEWithLogitsLoss is for binary decisions (survived/died) ‚Äî one output neuron.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "`BatchNorm1d`: normalises mini-batch activations ‚Üí reduces internal covariate shift ‚Üí allows higher LR. `BCEWithLogitsLoss` = sigmoid + BCE in one numerically stable step. `AdamW` = Adam with decoupled weight decay. `CosineAnnealingLR` = smooth lr schedule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 4-layer brain: detect patterns ‚Üí combine ‚Üí output one number (survived probability)\n",
    "# ‚öôÔ∏è Output = single logit; BCEWithLogitsLoss applies sigmoid internally ‚Äî never add sigmoid manually\n",
    "class TitanicNet(nn.Module):\n",
    "    \"\"\"Deep network for tabular binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features=11):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 1)   # output: single logit for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = TitanicNet(n_features=X_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()   # Binary cross-entropy (includes sigmoid)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6  Training with DataLoader\n",
    "\n",
    "### üß† Brain Analogy\n",
    "Same 5-step loop with DataLoader delivering batches automatically. AUC = a better score than accuracy for imbalanced data: \"do survivors score higher than non-survivors?\" Save best checkpoint ‚Äî not just last epoch.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "AUC-ROC: threshold-independent ranking metric. AUC=1 perfect, AUC=0.5 random. Move batches to device just-in-time to minimise peak GPU memory usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Auto-delivered batches, balanced sampling, save best checkpoint (not just final epoch)\n",
    "# ‚öôÔ∏è AUC measured at all thresholds simultaneously ‚Äî better than fixed-threshold accuracy\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_b, y_b in loader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_b)\n",
    "        loss   = criterion(logits, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * len(X_b)\n",
    "        preds   = (torch.sigmoid(logits) >= 0.5).long()\n",
    "        correct += (preds == y_b.long()).sum().item()\n",
    "        total   += len(X_b)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    for X_b, y_b in loader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        logits = model(X_b)\n",
    "        loss   = criterion(logits, y_b)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "\n",
    "        running_loss += loss.item() * len(X_b)\n",
    "        preds   = (probs >= 0.5).long()\n",
    "        correct += (preds == y_b.long()).sum().item()\n",
    "        total   += len(X_b)\n",
    "\n",
    "        all_probs.extend(probs.cpu().numpy().flatten())\n",
    "        all_labels.extend(y_b.cpu().numpy().flatten())\n",
    "\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return running_loss / total, correct / total, auc\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Run training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EPOCHS = 100\n",
    "history = {k: [] for k in ['tr_loss','tr_acc','vl_loss','vl_acc','vl_auc']}\n",
    "best_val_auc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_l, tr_a          = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    vl_l, vl_a, vl_auc = evaluate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['tr_loss'].append(tr_l);  history['tr_acc'].append(tr_a)\n",
    "    history['vl_loss'].append(vl_l);  history['vl_acc'].append(vl_a)\n",
    "    history['vl_auc'].append(vl_auc)\n",
    "\n",
    "    if vl_auc > best_val_auc:\n",
    "        best_val_auc = vl_auc\n",
    "        torch.save(model.state_dict(), 'titanic_best.pth')   # save best\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | tr_loss {tr_l:.4f} tr_acc {tr_a:.3f} | \"\n",
    "              f\"vl_loss {vl_l:.4f} vl_acc {vl_a:.3f} vl_AUC {vl_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7  Visualise & Evaluate\n",
    "\n",
    "### üß† Brain Analogy\n",
    "ROC curve: at each threshold, how many real survivors do we catch vs how many false alarms? AUC summarises this as one number. Perfect = top-left corner. Random = diagonal. Confusion matrix: where exactly did the model get confused?\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "ROC: TPR (recall for positive) vs FPR (1-specificity) at varying thresholds. AUC = probability that a random positive is ranked higher than a random negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Learning diary: fewer mistakes? More correct? AUC rising toward 1.0?\n",
    "# ‚öôÔ∏è Three subplots: loss, accuracy, AUC-ROC over training epochs\n",
    "# ‚îÄ‚îÄ Training curves ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['tr_loss'], label='Train')\n",
    "axes[0].plot(history['vl_loss'], label='Val')\n",
    "axes[0].set_title('Loss');  axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['tr_acc'], label='Train')\n",
    "axes[1].plot(history['vl_acc'], label='Val')\n",
    "axes[1].set_title('Accuracy'); axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['vl_auc'], color='purple')\n",
    "axes[2].set_title('Validation AUC-ROC')\n",
    "\n",
    "plt.suptitle('Titanic Survival Model ‚Äî Training History')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Load best state, plot decision quality: ROC curve + confusion matrix\n",
    "# ‚öôÔ∏è ROC = tradeoff at all thresholds | confusion matrix = specific errors at threshold=0.5\n",
    "# ‚îÄ‚îÄ Load best model & ROC curve ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "model.load_state_dict(torch.load('titanic_best.pth', map_location=device))\n",
    "_, val_acc, val_auc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "# Collect predictions for plots\n",
    "model.eval()\n",
    "all_probs, all_labels, all_preds = [], [], []\n",
    "with torch.no_grad():\n",
    "    for X_b, y_b in val_loader:\n",
    "        X_b = X_b.to(device)\n",
    "        probs = torch.sigmoid(model(X_b)).cpu().numpy().flatten()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(y_b.numpy().flatten())\n",
    "        all_preds.extend((probs >= 0.5).astype(int))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(fpr, tpr, lw=2, label=f'AUC = {val_auc:.4f}')\n",
    "ax1.plot([0,1],[0,1],'--', color='gray')\n",
    "ax1.set_xlabel('False Positive Rate'); ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve ‚Äî Titanic Survival'); ax1.legend()\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Died','Survived'], yticklabels=['Died','Survived'], ax=ax2)\n",
    "ax2.set_title(f'Confusion Matrix (acc={val_acc:.1%})')\n",
    "ax2.set_ylabel('True'); ax2.set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8  DataLoader Tips for Production\n",
    "\n",
    "### üß† Brain Analogy\n",
    "`num_workers=4` = 4 assistants preparing the next batch in parallel. `pin_memory=True` = pre-position cards in hand for instant GPU transfer. `prefetch_factor=2` = pre-load tomorrow's study material tonight.\n",
    "\n",
    "### ‚öôÔ∏è Engineer Analogy\n",
    "Production DataLoader: num_workers (parallel processes), pin_memory (non-pageable memory ‚Üí faster DMA), seed workers (reproducible augmentation), prefetch_factor (pre-load extra batches).\n",
    "\n",
    "```python\n",
    "# Large datasets: parallel loading\n",
    "DataLoader(dataset, batch_size=256, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Reproducibility: fix worker seeds\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(42 + worker_id)\n",
    "    random.seed(42 + worker_id)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "DataLoader(dataset, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "# Memory-efficient: prefetch_factor\n",
    "DataLoader(dataset, num_workers=4, prefetch_factor=2)\n",
    "```\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Download the real Titanic CSV from Kaggle and build a `TitanicDataset` that reads directly from file.\n",
    "2. Add a **threshold tuning** step: instead of 0.5, find the threshold that maximises F1 score.\n",
    "3. Implement **k-fold cross-validation** using `sklearn.model_selection.StratifiedKFold`.\n",
    "4. Use `torch.utils.tensorboard.SummaryWriter` to log losses and view in TensorBoard.\n",
    "\n",
    "---\n",
    "**Next ‚Üí** [Module 04: Convolutional Neural Networks ‚Äî CIFAR-10](./Module_04_CNNs_CIFAR10.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}