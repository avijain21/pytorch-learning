{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "---\n",
    "## How Does a Computer See?\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "When you look at a cat photo, your brain doesn't process every pixel separately:\n",
    "1. **V1** â†’ detects edges at different angles\n",
    "2. **V2, V4** â†’ combines edges into textures and shapes (fur, whiskers)\n",
    "3. **Inferotemporal cortex** â†’ recognises the whole object (\"cat!\")\n",
    "\n",
    "CNNs do the same, layer by layer. This is NOT a coincidence â€” CNNs were inspired by how the visual brain works (Hubel & Wiesel's Nobel Prize work on cat visual cortex).\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "CNNs use **learnable spatial filters** â€” like Sobel/Gaussian image processing but filter values are LEARNED from data. Key insight: **weight sharing** â€” the same 3Ã—3 filter slides ALL positions. Only 9 weights describe a pattern for the ENTIRE image, not millions.\n",
    "\n",
    "**Level:** Intermediate  \n",
    "**Duration:** ~4 hours  \n",
    "**Dataset:** CIFAR-10 (via `torchvision` â€” same as [Kaggle CIFAR-10](https://www.kaggle.com/c/cifar-10))  \n",
    "**Real-World Use Case:** Image classification for visual inspection, autonomous vehicles, medical imaging\n",
    "\n",
    "## What You'll Learn\n",
    "- CNN fundamentals: convolution, pooling, feature maps\n",
    "- Building CNN architectures with `nn.Conv2d`, `nn.MaxPool2d`, `nn.BatchNorm2d`\n",
    "- Image transforms with `torchvision.transforms`\n",
    "- Data augmentation to improve generalisation\n",
    "- Visualising learned filters and feature maps\n",
    "- Implementing a ResNet-style skip connection\n",
    "\n",
    "## CNN Intuition\n",
    "```\n",
    "Input Image (32Ã—32Ã—3)\n",
    "    â†“  Conv2d(3â†’32, kernel=3)  â†’ 32 feature maps detecting edges\n",
    "    â†“  ReLU + MaxPool2d(2)     â†’ 16Ã—16Ã—32  (spatial size halved)\n",
    "    â†“  Conv2d(32â†’64, kernel=3) â†’ 64 feature maps detecting textures\n",
    "    â†“  ReLU + MaxPool2d(2)     â†’ 8Ã—8Ã—64\n",
    "    â†“  Flatten â†’ Linear layers â†’ 10 class scores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Setting up tools and checking hardware for image processing\n",
    "# âš™ï¸ torchvision = pretrained models + standard datasets + image transforms built-in\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "CIFAR10_CLASSES = ('plane','car','bird','cat','deer',\n",
    "                   'dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1  Data Transforms & Augmentation\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Your brain recognises a cat whether it's on the left, in dim light, or slightly rotated. Data augmentation gives the model this same robustness by artificially varying training images: RandomCrop = \"partial view\", RandomHorizontalFlip = \"mirror is same object\", ColorJitter = \"different lighting.\"\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Augmentation = stochastic transforms applied ONLY during training. Each epoch applies a DIFFERENT random transform â†’ effectively infinite training data. Val transforms MUST be deterministic for fair, reproducible evaluation. Normalise with dataset-specific mean/std â†’ pixels in ~[-2,2] â†’ faster convergence.\n",
    "\n",
    "**Key insight:** Data augmentation is *applied only during training*, not validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Training: random variations teach robustness. Val: clean consistent images for fair evaluation.\n",
    "# âš™ï¸ RandomCrop(32, padding=4): pad to 40Ã—40, crop random 32Ã—32 â€” simulates different viewpoints\n",
    "# CIFAR-10 dataset statistics (pre-computed)\n",
    "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# â”€â”€ Training transforms (with augmentation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),          # random crop with padding\n",
    "    transforms.RandomHorizontalFlip(p=0.5),        # mirror 50% of the time\n",
    "    transforms.ColorJitter(brightness=0.2,\n",
    "                           contrast=0.2,\n",
    "                           saturation=0.2),        # color variation\n",
    "    transforms.ToTensor(),                         # PIL â†’ tensor [0,1]\n",
    "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),   # standardize\n",
    "])\n",
    "\n",
    "# â”€â”€ Validation transforms (deterministic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "])\n",
    "\n",
    "# â”€â”€ Download datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                         transform=train_transform, download=True)\n",
    "val_ds   = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                         transform=val_transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Look at sample images first â€” \"what are we asking the model to learn to recognise?\"\n",
    "# âš™ï¸ un-normalise: reverse (x-mean)/std to get pixel values back in [0,1] range for display\n",
    "# â”€â”€ Visualise sample images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def imshow(img_tensor, mean=CIFAR_MEAN, std=CIFAR_STD):\n",
    "    \"\"\"Un-normalise and show a tensor image.\"\"\"\n",
    "    img = img_tensor.numpy().transpose(1, 2, 0)\n",
    "    img = img * np.array(std) + np.array(mean)     # un-normalise\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "images, labels = next(iter(DataLoader(\n",
    "    torchvision.datasets.CIFAR10('./data', train=True,\n",
    "                                  transform=val_transform, download=False),\n",
    "    batch_size=16, shuffle=True\n",
    ")))\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for ax, img, lbl in zip(axes.flatten(), images, labels):\n",
    "    ax.imshow(imshow(img))\n",
    "    ax.set_title(CIFAR10_CLASSES[lbl], fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"CIFAR-10 Sample Images\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  CNN Architecture â€” Step by Step\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "A CNN layer = a team of visual detectors scanning the image: **Conv2d** asks \"is MY pattern HERE?\", **BatchNorm** normalises responses so no detector dominates, **ReLU** silences detectors that didn't find anything, **MaxPool2d(2)** keeps the STRONGEST detection from each 2Ã—2 patch and shrinks the map by half. MaxPool creates position-invariance: doesn't matter if the edge was at pixel (5,6) or (5,7).\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Conv output size: `H_out = (H_in + 2p - k)/s + 1`. With k=3, p=1, s=1: H_out = H_in. MaxPool(2) halves spatial dims. After 3 pools: 32â†’16â†’8â†’4. Channels grow: 3â†’32â†’64â†’128. Final tensor: 4Ã—4Ã—128=2048 values â†’ flattened for Linear layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  CNN = stacked teams of detectors, each layer finding more complex patterns than the last\n",
    "# âš™ï¸ ConvBlock = Conv2dâ†’BatchNormâ†’ReLU: standard building block. MaxPool halves spatial dimensions.\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv â†’ BatchNorm â†’ ReLU (the basic CNN building block).\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class CIFAR10Net(nn.Module):\n",
    "    \"\"\"Custom CNN for CIFAR-10 classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 32Ã—32Ã—3 â†’ 32Ã—32Ã—32\n",
    "            ConvBlock(3,  32),\n",
    "            ConvBlock(32, 32),\n",
    "            nn.MaxPool2d(2),          # â†’ 16Ã—16Ã—32\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            # Block 2: 16Ã—16Ã—32 â†’ 16Ã—16Ã—64\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 64),\n",
    "            nn.MaxPool2d(2),          # â†’ 8Ã—8Ã—64\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            # Block 3: 8Ã—8Ã—64 â†’ 8Ã—8Ã—128\n",
    "            ConvBlock(64,  128),\n",
    "            ConvBlock(128, 128),\n",
    "            nn.MaxPool2d(2),          # â†’ 4Ã—4Ã—128\n",
    "            nn.Dropout2d(0.4),\n",
    "        )\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 4 * 128, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CIFAR10Net().to(device)\n",
    "\n",
    "# Verify dimensions with a dummy pass\n",
    "dummy = torch.randn(4, 3, 32, 32).to(device)\n",
    "print(\"Output shape:\", model(dummy).shape)   # should be (4, 10)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3  ResNet-Style Skip Connection (Intermediate)\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "**Vanishing gradient problem**: error signal weakens as it travels back through many layers â€” early layers barely update. Skip connections create a **direct shortcut** from later layers back to earlier ones, like an express highway for the error signal. The \"+1\" in `âˆ‚(F(x)+x)/âˆ‚x = âˆ‚F/âˆ‚x + 1` keeps gradients alive even when `âˆ‚F/âˆ‚x â‰ˆ 0`.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Residual block: `out = F(x) + x`. Backprop: `âˆ‚out/âˆ‚x = âˆ‚F/âˆ‚x + 1`. The \"+1\" prevents vanishing gradients â€” why ResNet can have 50-150+ layers while plain networks degrade past ~20 layers.\n",
    "\n",
    "Skip connections solve the **vanishing gradient problem** in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Shortcut = express highway for error signal â€” bypasses vanishing gradient problem\n",
    "# âš™ï¸ out = F(x) + x â†’ gradient has additive identity path, stays strong through depth\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block: output = F(x) + x\"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x                         # save input\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + residual                 # skip connection (add input back)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Quick test\n",
    "rb = ResidualBlock(64)\n",
    "x  = torch.randn(2, 64, 16, 16)\n",
    "print(\"Residual block output shape:\", rb(x).shape)   # (2, 64, 16, 16) â€” unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4  Training the CNN\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "OneCycleLR = start with big confident steps (explore broadly), then take tiny careful steps at the end (fine-tune). Like hiking: big strides downhill, careful steps at the bottom. Label smoothing = \"don't be 100% sure â€” some CIFAR-10 photos are genuinely ambiguous.\"\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`label_smoothing=0.1`: replaces hard labels (0,1) with (0.1, 0.9) â€” prevents overconfidence. SGD + Nesterov: \"looks ahead\" before computing gradient. OneCycleLR: LR increases to max then cosine-decays to 0 â€” explores broadly then refines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Training setup: grader (loss), learning strategy (optimizer), schedule (when to slow down)\n",
    "# âš™ï¸ OneCycleLR requires total steps upfront: epochs Ã— steps_per_epoch\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)   # label smoothing = regularisation\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.1,\n",
    "    epochs=50, steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer, scheduler, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, correct, n = 0.0, 0, 0\n",
    "\n",
    "    ctx = torch.enable_grad() if train else torch.no_grad()\n",
    "    with ctx:\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss   = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item() * len(X)\n",
    "            correct    += (logits.argmax(1) == y).sum().item()\n",
    "            n          += len(X)\n",
    "\n",
    "    return total_loss / n, correct / n\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "history = defaultdict(list)\n",
    "best_acc = 0.0\n",
    "\n",
    "print(\"Training CNN on CIFAR-10 (this takes a few minutes)...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = run_epoch(model, train_loader, criterion, optimizer, scheduler, train=True)\n",
    "    vl_loss, vl_acc = run_epoch(model, val_loader,   criterion, optimizer, scheduler, train=False)\n",
    "\n",
    "    history['tr_loss'].append(tr_loss); history['tr_acc'].append(tr_acc)\n",
    "    history['vl_loss'].append(vl_loss); history['vl_acc'].append(vl_acc)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if vl_acc > best_acc:\n",
    "        best_acc = vl_acc\n",
    "        torch.save(model.state_dict(), 'cifar10_best.pth')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"Train {tr_acc:.3f} | Val {vl_acc:.3f} | Best {best_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Learning diary: loss down? Accuracy up? Learning rate schedule working as planned?\n",
    "# âš™ï¸ Three plots: loss (optimisation target), accuracy (performance), LR (schedule view)\n",
    "# â”€â”€ Learning curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['tr_loss'], label='Train'); axes[0].plot(history['vl_loss'], label='Val')\n",
    "axes[0].set_title('Loss'); axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['tr_acc'], label='Train'); axes[1].plot(history['vl_acc'], label='Val')\n",
    "axes[1].set_title('Accuracy'); axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['lr'])\n",
    "axes[2].set_title('Learning Rate (OneCycleLR)')\n",
    "\n",
    "plt.suptitle('CIFAR-10 CNN Training')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5  Visualise Learned Filters & Feature Maps\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "After training, we look inside: \"What patterns are you looking for?\" (filters) and \"What do you SEE in this image?\" (feature maps). First-layer CNN filters often look like Gabor wavelets â€” exactly what biological V1 neurons respond to. This convergence of biology and ML engineering is remarkable.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`register_forward_hook`: non-invasive callback that captures layer output during forward pass â€” no model modification needed. First conv weights shape: `(32, 3, 3, 3)` = 32 filters, each 3Ã—3 RGB. `register_forward_hook` is the standard tool for mechanistic interpretability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  \"What patterns is the first layer looking for?\" â€” like a V1 neuron preference map\n",
    "# âš™ï¸ first_conv.weight.data: (32,3,3,3). Normalise to [0,1] for display.\n",
    "# â”€â”€ First conv layer filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.load_state_dict(torch.load('cifar10_best.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "first_conv = model.features[0].block[0]   # first Conv2d\n",
    "filters = first_conv.weight.data.cpu()    # shape: (32, 3, 3, 3)\n",
    "\n",
    "# Normalize each filter to [0,1] for display\n",
    "filters_norm = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < filters_norm.shape[0]:\n",
    "        ax.imshow(filters_norm[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Learned Filters â€” First Convolutional Layer (3Ã—3 RGB)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  \"What does each detector SEE in THIS image?\" â€” like fMRI of activated brain regions\n",
    "# âš™ï¸ Forward hook captures activations; remove hook after use to avoid memory accumulation\n",
    "# â”€â”€ Feature maps for one image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sample_img, sample_lbl = val_ds[5]\n",
    "x = sample_img.unsqueeze(0).to(device)   # add batch dim\n",
    "\n",
    "# Hook to capture intermediate activations\n",
    "activations = {}\n",
    "def make_hook(name):\n",
    "    def hook(module, inp, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "h = model.features[0].block[2].register_forward_hook(make_hook('relu1'))\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "h.remove()\n",
    "\n",
    "feat_maps = activations['relu1'][0]   # shape: (32, 32, 32)\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < feat_maps.shape[0]:\n",
    "        ax.imshow(feat_maps[i].numpy(), cmap='viridis')\n",
    "    ax.axis('off')\n",
    "\n",
    "true_class = CIFAR10_CLASSES[sample_lbl]\n",
    "plt.suptitle(f\"Feature Maps after First ReLU â€” Image class: '{true_class}'\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6  Per-Class Accuracy Analysis\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Some objects are harder to recognise â€” we also find cat vs dog harder than ship vs truck. The model's confusion mirrors human visual confusion. Per-class analysis reveals where to improve: more data? Better augmentation? Harder negative mining?\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Per-class accuracy = `correct[i]/total[i]` for each class. Low accuracy = visually similar to others OR insufficient training examples. Guides data collection and targeted augmentation strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Where does the model struggle? Which classes does it confuse most?\n",
    "# âš™ï¸ Per-class accuracy = diagonal of normalised confusion matrix; compare to mean baseline\n",
    "class_correct = [0] * 10\n",
    "class_total   = [0] * 10\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in val_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        preds = model(X).argmax(1)\n",
    "        for pred, true in zip(preds.cpu(), y.cpu()):\n",
    "            class_correct[true] += (pred == true).item()\n",
    "            class_total[true]   += 1\n",
    "\n",
    "print(\"Per-class accuracy:\")\n",
    "accs = []\n",
    "for i, cls in enumerate(CIFAR10_CLASSES):\n",
    "    acc = 100 * class_correct[i] / class_total[i]\n",
    "    accs.append(acc)\n",
    "    print(f\"  {cls:8s}: {acc:5.1f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "bars = plt.bar(CIFAR10_CLASSES, accs, color='steelblue', edgecolor='white')\n",
    "plt.axhline(y=np.mean(accs), color='red', linestyle='--', label=f'Mean: {np.mean(accs):.1f}%')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Accuracy â€” CIFAR-10')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7  CNN Architecture Cheatsheet\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Quick reference for the visual brain's building blocks: Conv = edge/texture scanner, MaxPool = keep strongest signal and shrink, BatchNorm = normalise signals between layers, Dropout2d = silence entire feature maps for robustness, AdaptiveAvgPool = compress to fixed size.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "CNN output size formulas are essential for designing architectures. `AdaptiveAvgPool2d((1,1))` = global average pooling â€” produces one value per channel regardless of input size, the standard way to make CNNs input-size agnostic.\n",
    "\n",
    "| Layer | PyTorch | Output Size Formula |\n",
    "|-------|---------|--------------------|\n",
    "| Conv2d(in, out, k, s, p) | `nn.Conv2d` | âŒŠ(H+2p-k)/sâŒ‹+1 |\n",
    "| MaxPool2d(k, s) | `nn.MaxPool2d` | âŒŠ(H-k)/sâŒ‹+1 |\n",
    "| BatchNorm2d(C) | `nn.BatchNorm2d` | Same size |\n",
    "| Dropout2d(p) | `nn.Dropout2d` | Same size |\n",
    "| AdaptiveAvgPool2d((H,W)) | `nn.AdaptiveAvgPool2d` | Fixed HÃ—W |\n",
    "\n",
    "**Common CNN patterns:**\n",
    "- VGG style: stack 2-3 Conv â†’ MaxPool â†’ Dropout\n",
    "- ResNet style: 3Ã—3 Conv â†’ BN â†’ ReLU â†’ 3Ã—3 Conv â†’ BN + skip â†’ ReLU\n",
    "- Inception: parallel 1Ã—1, 3Ã—3, 5Ã—5 convs â†’ concatenate\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Reduce the model by removing one ConvBlock. How does accuracy change?\n",
    "2. Try replacing `MaxPool2d` with strided convolutions (`Conv2d(..., stride=2)`).\n",
    "3. Add the `ResidualBlock` class into `CIFAR10Net` between the conv blocks.\n",
    "4. Download the [Kaggle CIFAR-10](https://www.kaggle.com/c/cifar-10) dataset and generate predictions for submission.\n",
    "\n",
    "---\n",
    "**Next â†’** [Module 05: Transfer Learning with Pretrained Models](./Module_05_Transfer_Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}