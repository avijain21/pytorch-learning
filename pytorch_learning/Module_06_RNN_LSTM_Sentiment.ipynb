{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: RNNs, LSTMs & Sequential Data\n",
    "\n",
    "---\n",
    "## Why Do Sequences Need Special Treatment?\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "\"The movie was NOT good\" vs \"The movie was NOT bad\" â€” the word \"not\" flips the meaning completely. A regular neural network processes each word independently, missing that \"not\" changes what follows. Your brain reads sequentially with MEMORY â€” \"not\" stays in working memory and colours everything after it.\n",
    "\n",
    "RNNs and LSTMs do the same: process word by word, carrying a \"running understanding\" (hidden state) that updates with each new word.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Regular networks assume inputs are independent â€” wrong for text, time series, audio. Sequential data needs: (1) memory, (2) selective updating, (3) variable-length handling. RNN/LSTM/GRU solve all three.\n",
    "```\n",
    "RNN  â†’ one note (hidden state). Forgets early words (vanishing gradients).\n",
    "LSTM â†’ two memories: scratchpad (h) + diary (c). Diary keeps info for hundreds of steps.\n",
    "GRU  â†’ simplified LSTM, one combined memory. Fewer params, often same accuracy.\n",
    "```\n",
    "\n",
    "**Level:** Intermediate  \n",
    "**Duration:** ~4 hours  \n",
    "**Dataset:** IMDB Movie Reviews (via `datasets` library â€” same as [Kaggle IMDB sentiment](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews))  \n",
    "**Real-World Use Case:** Sentiment analysis, chatbots, time series forecasting, log anomaly detection\n",
    "\n",
    "## What You'll Learn\n",
    "- Text preprocessing: tokenization, vocabulary building, padding\n",
    "- Embedding layers (`nn.Embedding`)\n",
    "- Vanilla RNN, LSTM, and GRU â€” differences & when to use each\n",
    "- Bidirectional LSTMs\n",
    "- Packing padded sequences for efficiency\n",
    "- Bonus: 1D CNN for text (faster alternative)\n",
    "\n",
    "## The Problem with Regular Networks on Sequences\n",
    "```\n",
    "Review: \"The movie was not good, but I liked it anyway\"\n",
    "\n",
    "Regular NN: processes each word independently â†’ misses \"not good\" context\n",
    "\n",
    "RNN/LSTM:   maintains a hidden state (memory) â†’ understands \"not\" modifies \"good\"\n",
    "```\n",
    "\n",
    "## RNN vs LSTM vs GRU\n",
    "```\n",
    "RNN  â†’ simple, fast, suffers from vanishing gradients on long sequences\n",
    "LSTM â†’ adds cell state + 3 gates (forget/input/output), handles long dependencies\n",
    "GRU  â†’ simplified LSTM with 2 gates, often matches LSTM with fewer parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Tools for sequence processing â€” pack/pad for handling variable-length reviews efficiently\n",
    "# âš™ï¸ pack_padded_sequence + pad_packed_sequence: LSTM skips padding tokens for cleaner gradients\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1  Load IMDB Dataset\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Before the model can read a review, translate it: \"great movie!\" â†’ [\"great\", \"movie\"] â†’ [542, 17] â†’ embedding vectors. Each step makes the data more usable. Build the vocabulary from TRAINING data only â€” unseen test words map to UNK.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Vocabulary: bijective `wordâ†’int` mapping. MAX_VOCAB=20,000 covers ~95% of English. PAD=0 (placeholder), UNK=1 (out-of-vocabulary). Counter.most_common(N): efficient O(n log n) frequency ranking.\n",
    "\n",
    "```bash\n",
    "# Option A â€” Hugging Face datasets:\n",
    "pip install datasets\n",
    "\n",
    "# Option B â€” Kaggle:\n",
    "kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Load IMDB movie reviews â€” 25k positive, 25k negative â€” from Hugging Face\n",
    "# âš™ï¸ datasets library is the standard for loading NLP benchmarks without manual download\n",
    "# â”€â”€ Load from Hugging Face (no Kaggle account needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    hf_data = load_dataset('imdb', split={'train': 'train', 'test': 'test'})\n",
    "    train_texts  = hf_data['train']['text']\n",
    "    train_labels = hf_data['train']['label']\n",
    "    test_texts   = hf_data['test']['text']\n",
    "    test_labels  = hf_data['test']['label']\n",
    "    print(\"Loaded from Hugging Face datasets\")\n",
    "except ImportError:\n",
    "    # Fallback: torchtext built-in IMDB\n",
    "    import torchtext\n",
    "    train_data, test_data = torchtext.datasets.IMDB(root='./data')\n",
    "    train_labels, train_texts = zip(*list(train_data))\n",
    "    test_labels,  test_texts  = zip(*list(test_data))\n",
    "    train_labels = [1 if l == 'pos' else 0 for l in train_labels]\n",
    "    test_labels  = [1 if l == 'pos' else 0 for l in test_labels]\n",
    "    print(\"Loaded from torchtext\")\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n",
    "print(f\"\\nSample review (first 200 chars):\\n{train_texts[0][:200]}...\")\n",
    "print(f\"\\nLabel: {train_labels[0]}  (1=positive, 0=negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2  Text Preprocessing & Vocabulary\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Encode: translate each review to word-ID numbers. Padding challenge: reviews have DIFFERENT lengths but batches need FIXED size. Solution: add 0s (blank tokens) to short reviews â€” like blank pages at the end of short chapters. `lengths` tensor remembers the TRUE length so the LSTM can stop processing at the right point.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`pad_sequence`: takes variable-length tensors â†’ padded matrix shape (B, max_len). `lengths`: true pre-padding length per sequence â€” needed for `pack_padded_sequence` to skip PAD tokens during LSTM computation. `padding_value=0`: PAD â†’ zero embedding â†’ zero gradient contribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Build a word-to-number dictionary from training reviews\n",
    "# âš™ï¸ Counter.most_common: O(n log n) frequency counting; top 20k words covers 95% of text\n",
    "# â”€â”€ Tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def tokenize(text: str) -> list:\n",
    "    \"\"\"Simple whitespace tokenizer with basic cleaning.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)              # remove HTML tags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# â”€â”€ Build vocabulary from training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_VOCAB  = 20_000\n",
    "MAX_SEQ_LEN = 256   # truncate/pad to this length\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN, UNK_TOKEN = '<PAD>', '<UNK>'\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "vocab.update({\n",
    "    word: idx + 2\n",
    "    for idx, (word, _) in enumerate(counter.most_common(MAX_VOCAB - 2))\n",
    "})\n",
    "PAD_IDX = vocab[PAD_TOKEN]\n",
    "UNK_IDX = vocab[UNK_TOKEN]\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"Most common words: {list(counter.most_common(10))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Translate reviews to ID sequences; pad short ones so all fit in the same batch\n",
    "# âš™ï¸ collate_fn: called by DataLoader to combine variable-length sequences into fixed batches\n",
    "def encode(text: str, vocab: dict, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"Tokenize â†’ numericalize â†’ truncate/pad.\"\"\"\n",
    "    tokens = tokenize(text)[:max_len]\n",
    "    ids    = [vocab.get(t, UNK_IDX) for t in tokens]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.encodings = [encode(t, vocab, max_len) for t in texts]\n",
    "        self.labels    = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences in a batch to equal length.\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths  = torch.tensor([len(s) for s in sequences])\n",
    "    padded   = pad_sequence(sequences, batch_first=True, padding_value=PAD_IDX)\n",
    "    return padded, torch.stack(list(labels)), lengths\n",
    "\n",
    "\n",
    "# Use 5k train + 2k test for speed (increase for better results)\n",
    "N_TRAIN, N_TEST = 5000, 2000\n",
    "train_ds = IMDBDataset(train_texts[:N_TRAIN], train_labels[:N_TRAIN], vocab, MAX_SEQ_LEN)\n",
    "test_ds  = IMDBDataset(test_texts[:N_TEST],  test_labels[:N_TEST],  vocab, MAX_SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64,  shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "# Inspect a batch\n",
    "x_b, y_b, lens = next(iter(train_loader))\n",
    "print(f\"Batch X: {x_b.shape}, Y: {y_b.shape}, Lengths: {lens[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3  Understanding Embeddings\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "An embedding gives each word a position in a \"meaning space\" where similar words are close together. \"cat\" and \"dog\" are nearby (both pets). \"happy\" and \"joyful\" are almost identical. The model LEARNS these positions during training â€” words appearing in similar contexts end up close in embedding space.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`nn.Embedding(vocab_size, embed_dim, padding_idx=0)` = (N, D) learnable lookup table. Row lookup is O(1). padding_idx=0: row 0 = all zeros, no gradient contribution. Initialise with pretrained GloVe/Word2Vec via `nn.Embedding.from_pretrained(vectors)` for better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Each word-ID â†’ an 8-dimensional \"meaning vector\" in semantic space\n",
    "# âš™ï¸ Embedding = learned lookup table (vocab_size Ã— embed_dim). Padding idx=0 â†’ zeros, no gradient.\n",
    "# nn.Embedding: lookup table of shape (vocab_size, embedding_dim)\n",
    "# Maps integer token IDs â†’ dense vectors\n",
    "\n",
    "emb = nn.Embedding(num_embeddings=100, embedding_dim=8, padding_idx=0)\n",
    "token_ids = torch.tensor([[1, 5, 3, 0, 0],    # batch item 1 (padded at end)\n",
    "                           [2, 7, 4, 6, 9]])   # batch item 2\n",
    "embedded = emb(token_ids)\n",
    "print(f\"Input shape:  {token_ids.shape}\")\n",
    "print(f\"Output shape: {embedded.shape}\")\n",
    "print(\"Each token mapped to 8-dim vector\")\n",
    "print(f\"Padding idx (0) embedding: {emb(torch.tensor([0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4  Model Architectures\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "**LSTM**: two memories. \"The movie was, despite the terrible reviews, actually quite good.\" By \"good\" you need: short-term (\"actually quite\") AND long-term (\"despite terrible reviews\"). LSTM's cell state (c) = the diary that holds long-range context. Three gates: forget (erase stale info), input (remember important new words), output (what to use right now).\n",
    "\n",
    "**GRU**: combines forget+input into one update gate. Simpler, fewer params, good default.\n",
    "\n",
    "**Bidirectional**: reads sentence forwards AND backwards â€” gets context from both directions for each word.\n",
    "\n",
    "**TextCNN**: scans ALL windows of 3/4/5 words simultaneously. Very fast (fully parallel) but limited long-range context.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "LSTM cell: `c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t`. Additive update = direct gradient highway â†’ no vanishing. `pack_padded_sequence` = skip PAD tokens â†’ cleaner gradients. `clip_grad_norm_(1.0)` = cap total gradient norm to prevent BPTT explosion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Three reading strategies: word-by-word-with-memory (LSTM/GRU) vs parallel-scanner (TextCNN)\n",
    "# âš™ï¸ Bidirectional LSTM: concat hidden[-2] (forward) and hidden[-1] (backward) for full context\n",
    "# â”€â”€ 1. Simple LSTM Sentiment Classifier â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM for text classification.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256,\n",
    "                 n_layers=2, bidirectional=True, dropout=0.5, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim,\n",
    "                                       padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Bidirectional â†’ 2Ã— hidden_dim\n",
    "        in_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)   # binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len), lengths: actual lengths before padding\n",
    "\n",
    "        embedded = self.dropout(self.embedding(x))  # (B, L, embed_dim)\n",
    "\n",
    "        # Pack padded sequences for efficiency\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(),\n",
    "                                       batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed)\n",
    "\n",
    "        # Take last hidden state from both directions\n",
    "        # hidden: (n_layers * n_directions, B, hidden_dim)\n",
    "        hidden = self.dropout(torch.cat(\n",
    "            [hidden[-2], hidden[-1]], dim=1   # last layer, both directions\n",
    "        ))   # (B, hidden_dim*2)\n",
    "\n",
    "        return self.classifier(hidden).squeeze(1)   # (B,)\n",
    "\n",
    "\n",
    "# â”€â”€ 2. GRU variant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"GRU-based classifier â€” fewer params than LSTM, often similar performance.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256,\n",
    "                 n_layers=2, bidirectional=True, dropout=0.5, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim, hidden_dim, n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        in_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.dropout(self.embedding(x))\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(),\n",
    "                                       batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.gru(packed)\n",
    "        out = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        return self.fc(self.dropout(out)).squeeze(1)\n",
    "\n",
    "\n",
    "# â”€â”€ 3. 1D CNN (fast alternative to RNN) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"Text CNN with multiple kernel sizes (Kim 2014).\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=128, n_filters=100,\n",
    "                 filter_sizes=(3, 4, 5), dropout=0.5, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, n_filters, fs) for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        emb = self.embedding(x).permute(0, 2, 1)   # (B, embed, L)\n",
    "        pooled = [torch.relu(conv(emb)).max(dim=2)[0] for conv in self.convs]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat).squeeze(1)\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "lstm_model = LSTMClassifier(VOCAB_SIZE, pad_idx=PAD_IDX).to(device)\n",
    "gru_model  = GRUClassifier(VOCAB_SIZE,  pad_idx=PAD_IDX).to(device)\n",
    "cnn_model  = TextCNN(VOCAB_SIZE,        pad_idx=PAD_IDX).to(device)\n",
    "\n",
    "for name, m in [('LSTM', lstm_model), ('GRU', gru_model), ('TextCNN', cnn_model)]:\n",
    "    params = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"{name}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5  Training Loop\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Same 5-step training loop, plus gradient clipping â€” a circuit breaker for the error signal. Without it, errors travelling back through many time steps can explode exponentially. `clip_grad_norm_(1.0)` = \"if the error signal gets too strong, cap it at 1.\"\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`clip_grad_norm_(params, 1.0)`: clips TOTAL gradient norm to 1.0. Critical for BPTT â€” without it gradients grow exponentially through long sequences. Standard practice for ALL RNN training. Never skip this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Train all three models on same reviews â€” who learns sentiment best?\n",
    "# âš™ï¸ clip_grad_norm_(1.0): CRITICAL for RNNs â€” prevents exploding gradients through long sequences\n",
    "def train_text_model(model, train_loader, test_loader, epochs=10, lr=1e-3, name='model'):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "    history = {'tr_acc': [], 'vl_acc': []}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        for X, y, lens in train_loader:\n",
    "            X, y, lens = X.to(device), y.to(device), lens\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X, lens)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # prevent exploding gradients\n",
    "            optimizer.step()\n",
    "            preds   = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == y.long()).sum().item()\n",
    "            total   += len(y)\n",
    "        tr_acc = correct / total\n",
    "\n",
    "        # Eval\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y, lens in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds = (torch.sigmoid(model(X, lens)) >= 0.5).long()\n",
    "                correct += (preds == y.long()).sum().item()\n",
    "                total   += len(y)\n",
    "        vl_acc = correct / total\n",
    "        scheduler.step(1 - vl_acc)\n",
    "\n",
    "        history['tr_acc'].append(tr_acc)\n",
    "        history['vl_acc'].append(vl_acc)\n",
    "\n",
    "        if vl_acc > best_acc:\n",
    "            best_acc = vl_acc\n",
    "            torch.save(model.state_dict(), f'{name}_best.pth')\n",
    "\n",
    "        print(f\"[{name}] Epoch {epoch:2d}/{epochs} | \"\n",
    "              f\"train {tr_acc:.3f} | test {vl_acc:.3f}\")\n",
    "\n",
    "    return history, best_acc\n",
    "\n",
    "\n",
    "# Train all three\n",
    "results = {}\n",
    "for name, model in [('LSTM', lstm_model), ('GRU', gru_model), ('TextCNN', cnn_model)]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}\")\n",
    "    history, best = train_text_model(model, train_loader, test_loader,\n",
    "                                      epochs=10, name=name)\n",
    "    results[name] = {'history': history, 'best': best}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6  Compare Models\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Three different reading strategies, one test. LSTM/GRU handle \"not good\" and \"actually quite good despite terrible reviews\" better (long-range context). TextCNN is fastest. All three achieve ~85-90% on IMDB. For state-of-the-art: BERT/GPT (transformers) â€” that's the next frontier.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "TextCNN: fully parallelisable â†’ fast GPU training. LSTM/GRU: sequential time dependency â†’ can't parallelise across sequence. Transformers: parallel attention over all positions â†’ best accuracy + parallelism, but need more data/memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Side-by-side: sequential reader (LSTM/GRU) vs parallel scanner (TextCNN) â€” who wins?\n",
    "# âš™ï¸ Solid line = test accuracy; dashed = train; bar chart shows best test accuracy per model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
    "colors = {'LSTM': 'blue', 'GRU': 'orange', 'TextCNN': 'green'}\n",
    "\n",
    "for name, data in results.items():\n",
    "    ax1.plot(data['history']['tr_acc'], '--', color=colors[name], alpha=0.5)\n",
    "    ax1.plot(data['history']['vl_acc'],  '-', color=colors[name], label=name)\n",
    "\n",
    "ax1.set_title('Test Accuracy (solid) vs Train (dashed)')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "best_accs = {k: v['best'] for k, v in results.items()}\n",
    "ax2.bar(best_accs.keys(), best_accs.values(),\n",
    "        color=[colors[k] for k in best_accs])\n",
    "for i, (k, v) in enumerate(best_accs.items()):\n",
    "    ax2.text(i, v + 0.005, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.set_title('Best Test Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "plt.suptitle('IMDB Sentiment Analysis â€” Model Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7  Inference â€” Predict Sentiment\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "The trained LSTM reads a new review word by word, building a sentiment signal. By the end: \"> 0.5 = positive, < 0.5 = negative.\" Confidence = how far from 0.5. \"0.95 = very sure positive.\" \"0.52 = barely positive, model is uncertain.\"\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "Inference: tokenize â†’ encode â†’ `unsqueeze(0)` (add batch dim) â†’ `model.eval()` forward â†’ `sigmoid(logit)` â†’ threshold at 0.5. `torch.no_grad()` = no gradient tracking needed â†’ ~30% faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Feed new unseen reviews to the trained brain â€” what does it think about each one?\n",
    "# âš™ï¸ unsqueeze(0) adds batch dim; sigmoid converts logit to probability [0,1]\n",
    "# Load best LSTM model\n",
    "lstm_model.load_state_dict(torch.load('LSTM_best.pth', map_location=device))\n",
    "lstm_model.eval()\n",
    "\n",
    "def predict_sentiment(text: str, model, vocab: dict, max_len: int) -> dict:\n",
    "    tokens = encode(text, vocab, max_len).unsqueeze(0).to(device)  # (1, L)\n",
    "    length = torch.tensor([tokens.shape[1]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = model(tokens, length)\n",
    "        prob  = torch.sigmoid(logit).item()\n",
    "\n",
    "    return {\n",
    "        'text': text[:80] + '...',\n",
    "        'probability': prob,\n",
    "        'sentiment': 'POSITIVE' if prob >= 0.5 else 'NEGATIVE',\n",
    "        'confidence': max(prob, 1 - prob)\n",
    "    }\n",
    "\n",
    "\n",
    "test_reviews = [\n",
    "    \"Absolutely brilliant film! The acting was outstanding and the story was gripping.\",\n",
    "    \"Terrible waste of time. The plot made no sense and the acting was wooden.\",\n",
    "    \"It started slow but picked up midway. Not bad overall, decent entertainment.\",\n",
    "    \"This movie changed my life. A masterpiece of modern cinema.\",\n",
    "    \"I walked out after 30 minutes. Unwatchable garbage.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, lstm_model, vocab, MAX_SEQ_LEN)\n",
    "    icon = 'ğŸ˜Š' if result['sentiment'] == 'POSITIVE' else 'ğŸ˜'\n",
    "    print(f\"{icon} [{result['sentiment']:8s} {result['confidence']:.0%}] \"\n",
    "          f\"{result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8  Understanding RNN Internals\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Watch the LSTM's memory change word by word as it reads a sentence. Hidden units activate differently for \"terribly\" vs \"redeemed\". The sentiment signal shifts after \"but\" â€” the model re-evaluates. Like watching your reading comprehension unfold in slow motion.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`register_forward_hook` captures packed LSTM output. `pad_packed_sequence` unpacks to (seq_len, hidden*2). Heatmap: each row = one hidden unit, each column = one word position. Mechanistic interpretability â€” understanding what internal representations were learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Watch the brain's memory change word-by-word â€” see sentiment shift after \"but\"\n",
    "# âš™ï¸ Hook captures packed output â†’ unpack â†’ slice first 32 hidden units â†’ heatmap\n",
    "# â”€â”€ Anatomy of LSTM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2,\n",
    "               batch_first=True, bidirectional=False)\n",
    "\n",
    "# Input: (batch, seq_len, input_size)\n",
    "x = torch.randn(4, 15, 10)   # 4 sequences, length 15, 10 features\n",
    "output, (hidden, cell) = lstm(x)\n",
    "\n",
    "print(\"LSTM shapes:\")\n",
    "print(f\"  Input          : {x.shape}\")\n",
    "print(f\"  Output         : {output.shape}  â† all timestep outputs\")\n",
    "print(f\"  Hidden (h_n)   : {hidden.shape}  â† final hidden state (n_layers, B, H)\")\n",
    "print(f\"  Cell   (c_n)   : {cell.shape}    â† final cell state\")\n",
    "print()\n",
    "\n",
    "# â”€â”€ GRU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "output_gru, hidden_gru = gru(x)\n",
    "print(\"GRU shapes:\")\n",
    "print(f\"  Output : {output_gru.shape}\")\n",
    "print(f\"  Hidden : {hidden_gru.shape}  â† GRU has no cell state (simpler!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Peek at raw LSTM machinery â€” what shapes come out and what does each represent?\n",
    "# âš™ï¸ output=(B,L,H*dirs), h_n=(layers*dirs,B,H), c_n=same. GRU has no cell state (simpler!).\n",
    "# â”€â”€ Visualise hidden state evolution across tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "lstm_model.eval()\n",
    "review = \"The movie started terribly but the ending completely redeemed it\"\n",
    "tokens = encode(review, vocab, MAX_SEQ_LEN).unsqueeze(0).to(device)\n",
    "length = torch.tensor([tokens.shape[1]])\n",
    "\n",
    "# Attach hook to capture hidden states at each timestep\n",
    "all_outputs = []\n",
    "def hook_fn(module, inp, out):\n",
    "    packed_out, _ = out\n",
    "    all_outputs.append(packed_out)\n",
    "\n",
    "h = lstm_model.lstm.register_forward_hook(hook_fn)\n",
    "with torch.no_grad():\n",
    "    _ = lstm_model(tokens, length)\n",
    "h.remove()\n",
    "\n",
    "# Unpack outputs to get per-token hidden states\n",
    "packed_out = all_outputs[0]\n",
    "unpacked, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "hidden_states = unpacked[0].cpu().numpy()   # (seq_len, hidden*2)\n",
    "\n",
    "word_tokens = tokenize(review)[:MAX_SEQ_LEN]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(hidden_states[:len(word_tokens), :32].T, aspect='auto',\n",
    "           cmap='RdBu', vmin=-1, vmax=1)\n",
    "plt.xticks(range(len(word_tokens)), word_tokens, rotation=45, ha='right')\n",
    "plt.ylabel('Hidden unit')\n",
    "plt.title('LSTM Hidden State Activations per Token (first 32 units)')\n",
    "plt.colorbar(label='Activation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9  Architecture Comparison Table\n",
    "\n",
    "### ğŸ§  Brain Analogy\n",
    "Summary of all sequential reading strategies: RNN (simple, forgets), LSTM (full memory), GRU (simplified memory), TextCNN (fast parallel scanner), Transformer (state-of-the-art attention). Each tool has its use case.\n",
    "\n",
    "### âš™ï¸ Engineer Analogy\n",
    "`clip_grad_norm_` is ALWAYS needed for RNNs. `pack_padded_sequence` = standard efficiency for variable-length sequences. Transformers replaced RNNs for most NLP but RNNs still shine for real-time streaming data.\n",
    "\n",
    "| Model | Pros | Cons | Best For |\n",
    "|-------|------|------|----------|\n",
    "| RNN | Simple, fast | Vanishing gradients on long seq | Very short sequences |\n",
    "| LSTM | Long-range dependencies | Slower, more params than GRU | Long text, time series |\n",
    "| GRU | Faster than LSTM, fewer params | Slightly worse on very long seq | General purpose sequences |\n",
    "| TextCNN | Very fast, parallelizable | Limited long-range context | Short texts, high throughput |\n",
    "| Transformer | Best accuracy, parallelizable | Memory O(nÂ²), needs lots of data | State-of-the-art NLP |\n",
    "\n",
    "## Sequential Data Cheatsheet\n",
    "\n",
    "```python\n",
    "# LSTM\n",
    "nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n",
    "         bidirectional=True, dropout=0.5)\n",
    "\n",
    "# GRU\n",
    "nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "# Pack padded (efficiency when sequences have different lengths)\n",
    "packed = pack_padded_sequence(embedded, lengths, batch_first=True,\n",
    "                               enforce_sorted=False)\n",
    "output, (h_n, c_n) = lstm(packed)\n",
    "output, lengths = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "# Gradient clipping (CRITICAL for RNNs â€” prevents exploding gradients)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Download the [Kaggle IMDB dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) and train on all 50k reviews.\n",
    "2. Add **pre-trained GloVe embeddings** (`nn.Embedding.from_pretrained(glove_vectors)`) instead of random init.\n",
    "3. Apply this same pipeline to **time series prediction** â€” replace text tokens with stock price windows.\n",
    "4. Upgrade to a simple **Transformer encoder** using `nn.TransformerEncoder`. Compare accuracy.\n",
    "\n",
    "---\n",
    "## Congratulations! You've completed the PyTorch Learning Path!\n",
    "\n",
    "### Next Steps\n",
    "- Explore **Hugging Face Transformers** for state-of-the-art NLP\n",
    "- Learn **PyTorch Lightning** for clean, scalable training code\n",
    "- Try **Weights & Biases (wandb)** for experiment tracking\n",
    "- Study **model deployment** with TorchServe or FastAPI\n",
    "- Enter a [Kaggle competition](https://www.kaggle.com/competitions) to apply your skills!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}